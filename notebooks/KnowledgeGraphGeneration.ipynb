{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Knowledge Graph Generation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n",
      "/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/local/python-env/py39/lib/python3.9/site-packages\")\n",
    "\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialze a SparkSession\n",
    "\n",
    "Initialize a test session to ensure the SparkSession is working properly. This will connect to the resource manager node that is running the YARN cluster. If we visit the YARN web portal, we can see that the Spark application is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring the pyspark library is being accessed from my local usr directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Sedona version: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "sedona_version = pkg_resources.get_distribution(\"apache-sedona\").version\n",
    "print(f\"Apache Sedona version: {sedona_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/latest\n",
      "/usr/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['PYSPARK_PYTHON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SedonaKepler import, verify if keplergl is installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://artifacts.unidata.ucar.edu/repository/unidata-all added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /s/chopin/a/grad/flarrieu/.ivy2/cache\n",
      "The jars for the packages stored in: /s/chopin/a/grad/flarrieu/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-3.5_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ce567161-90a8-42ed-9e56-7bf7b65986bb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-3.5_2.12;1.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-common;1.5.1 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/3.5.0-with-hadoop3.3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.19.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.google.guava#guava;25.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in user-list\n",
      "\tfound org.checkerframework#checker-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound com.uber#h3;4.1.1 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.10.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-spark-common-3.5_2.12;1.5.1 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in user-list\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.5.1-28.2 in central\n",
      ":: resolution report :: resolve 528ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from user-list in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;25.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.uber#h3;4.1.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from user-list in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-common;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-3.5_2.12;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-common-3.5_2.12;1.5.1 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.5.1-28.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n",
      "\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   23  |   1   |   1   |   2   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ce567161-90a8-42ed-9e56-7bf7b65986bb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/10ms)\n",
      "24/04/17 12:41:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://columbus-oh.cs.colostate.edu:30800\").appName(\"MyApp\").config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "            'org.apache.sedona:sedona-spark-3.5_2.12:1.5.1,'\n",
    "            'org.datasyslab:geotools-wrapper:1.5.1-28.2') \\\n",
    "    .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_652923/4046572303.py:3: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(spark)\n",
      "/s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages/sedona/register/geo_registrator.py:45: DeprecationWarning: Call to deprecated function register (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  cls.register(spark)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sedona.register import SedonaRegistrator\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to make the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 12:42:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "/tmp/ipykernel_652923/4152555376.py:35: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(spark)\n",
      "/s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages/sedona/register/geo_registrator.py:45: DeprecationWarning: Call to deprecated function register (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  cls.register(spark)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.functions import year  # used to extract year from date, could do this manually as well\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum as pyspark_sum\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "import geopandas as gpd\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('GeoSpatialQueries_Freddy') \\\n",
    "    .master('spark://columbus-oh.cs.colostate.edu:30801') \\\n",
    "    .config(\"spark.yarn.resourcemanager.address\", \"columbia.cs.colostate.edu:30799\") \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "            'org.apache.sedona:sedona-spark-3.5_2.12:1.5.1,'\n",
    "            'org.datasyslab:geotools-wrapper:1.5.1-28.2') \\\n",
    "    .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to DEBUG\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# create a logger\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "logger.info(\"Pyspark initialized...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets\n",
    "\n",
    "Got the code for this from https://sedona.apache.org/1.5.1/tutorial/sql/\n",
    "\n",
    "Load GeoJSON using Spark JSON Data Source:\n",
    "\n",
    "Spark SQL's built-in JSON data source supports reading GeoJSON data. To ensure proper parsing of the geometry property, we can define a schema with the geometry property set to type 'string'. This prevents Spark from interpreting the property and allows us to use the ST_GeomFromGeoJSON function for accurate geometry parsing.\n",
    "\n",
    "```python\n",
    "schema = \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\";\n",
    "(sedona.read.json(geojson_path, schema=schema)\n",
    "    .selectExpr(\"explode(features) as features\") # Explode the envelope to get one feature per row.\n",
    "    .select(\"features.*\") # Unpack the features struct.\n",
    "    .withColumn(\"geometry\", f.expr(\"ST_GeomFromGeoJSON(geometry)\")) # Convert the geometry string.\n",
    "    .printSchema())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module from py4j to interact with JVM\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Import the Path class from Hadoop. This class is used to handle file paths in Hadoop.\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "\n",
    "# Define a function to recursively get all .json and .geojson files in a directory and its subdirectories\n",
    "def get_files_recursive(path):\n",
    "    # Use the listStatus method of the FileSystem class to get an array of FileStatus objects\n",
    "    # Each FileStatus object represents a file or directory in the given path\n",
    "    file_status_arr = fs.listStatus(spark._jvm.Path(path))\n",
    "    \n",
    "    # Initialize an empty list to hold the file paths\n",
    "    file_paths = []\n",
    "    \n",
    "    # Loop through each FileStatus object in the array\n",
    "    for file_status in file_status_arr:\n",
    "        # If the FileStatus object represents a directory\n",
    "        if file_status.isDirectory():\n",
    "            # Call the get_files_recursive function with the directory path\n",
    "            # This is a recursive call, which means the function calls itself\n",
    "            # Add the returned file paths to the file_paths list\n",
    "            file_paths += get_files_recursive(file_status.getPath().toString())\n",
    "        # If the FileStatus object represents a file that ends with .json or .geojson\n",
    "        elif file_status.getPath().getName().endswith(('.json', '.geojson')):\n",
    "            # Add the file path to the file_paths list\n",
    "            file_paths.append(file_status.getPath().toString())\n",
    "    \n",
    "    print(file_paths)\n",
    "    # Return the list of file paths\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/AgriculturalArea.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Areas_diapiric_structures.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Bathymetry.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Calderas.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/CityBoundaries.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/CountryTerritories.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Diapiric_trends.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Diapirs.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Dikes_sills.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/EcoRegions.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Faults.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Gas_fluid_seep.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Gas_oil_seep.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/GeographicRegions.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Geologic_contacts.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Geologic_overprints.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Geologic_units.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Glaciation_extents.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Hydrothermal_vents.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Impact_structure_great_10_KM.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Impact_structure_less_10_KM.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Maganifeorus_deposits.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Phosphate_nodules_pavement.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Polymetallic_sulfide_deposits.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Rock_in_seafloor_sample.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Special_submarine_features.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Unusual_igneous_rocks.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Volcanoes.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/WorldContinents.geojson']\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/AgriculturalArea.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Areas_diapiric_structures.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Bathymetry.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Calderas.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/CityBoundaries.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/CountryTerritories.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Diapiric_trends.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Diapirs.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Dikes_sills.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/EcoRegions.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Faults.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Gas_fluid_seep.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Gas_oil_seep.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/GeographicRegions.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Geologic_contacts.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Geologic_overprints.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Geologic_units.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Glaciation_extents.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Hydrothermal_vents.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Impact_structure_great_10_KM.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Impact_structure_less_10_KM.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Maganifeorus_deposits.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Phosphate_nodules_pavement.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Polymetallic_sulfide_deposits.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Rock_in_seafloor_sample.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Special_submarine_features.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Unusual_igneous_rocks.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/Volcanoes.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/WorldContinents.geojson\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a Hadoop file system \n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "\n",
    "# Directory containing the files\n",
    "json_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/\"\n",
    "\n",
    "# Define the schema for the GeoJSON data\n",
    "geojsonSchema = \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "\n",
    "# Get a list of the JSON and GeoJSON files in the directory and its subdirectories\n",
    "json_files = get_files_recursive(json_directory)\n",
    "\n",
    "# Create a dictionary to hold the DataFrames\n",
    "json_dataset_dataframes = {}\n",
    "\n",
    "# Define the current and desired EPSG codes\n",
    "current_epsg = \"EPSG:3857\"  # Web Mercator\n",
    "desired_epsg = \"EPSG:4326\"  # WGS84\n",
    "\n",
    "# Load each JSON file into a DataFrame and store it in the dictionary\n",
    "for file_path in json_files:\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    \n",
    "    # Print the file path\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    # Read the GeoJSON file using the defined schema using sedona into a spark dataframe\n",
    "    df = spark.read.schema(geojsonSchema).json(file_path, multiLine=True)\n",
    "    \n",
    "    # Explode the features array to create a row for each feature and select the columns\n",
    "    df = (df\n",
    "        .select(F.explode(\"features\").alias(\"features\"))\n",
    "        .select(\"features.*\")\n",
    "        # Use Sedona's ST_GeomFromGeoJSON function to convert the geometry string to a geometry object\n",
    "        .withColumn(\"geometry\", F.expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "        # Transform the 'geometry' column from the current EPSG code to the desired EPSG code\n",
    "        .withColumn(\"geometry\", F.expr(f\"ST_Transform(geometry, '{current_epsg}', '{desired_epsg}')\"))\n",
    "        )\n",
    "    \n",
    "    json_dataset_dataframes[file_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the datasets are not in a workable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgriculturalArea.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Areas_diapiric_structures.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Bathymetry.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Calderas.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "CityBoundaries.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "CountryTerritories.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Diapiric_trends.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Diapirs.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Dikes_sills.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "EcoRegions.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Faults.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Gas_fluid_seep.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Gas_oil_seep.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "GeographicRegions.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Geologic_contacts.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Geologic_overprints.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Geologic_units.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Glaciation_extents.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Hydrothermal_vents.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Impact_structure_great_10_KM.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Impact_structure_less_10_KM.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Maganifeorus_deposits.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Phosphate_nodules_pavement.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Polymetallic_sulfide_deposits.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Rock_in_seafloor_sample.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Special_submarine_features.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Unusual_igneous_rocks.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "Volcanoes.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "WorldContinents.geojson Schema:\n",
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schemas\n",
    "\n",
    "for key, value in json_dataset_dataframes.items():\n",
    "    print(f\"{key} Schema:\")\n",
    "    value.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
