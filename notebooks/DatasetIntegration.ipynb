{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "import pkg_resources\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.functions import year  # used to extract year from date, could do this manually as well\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum as pyspark_sum\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "import geopandas as gpd\n",
    "from py4j.java_gateway import java_import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n",
      "/usr/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/usr/local/python-env/py39/lib/python3.9/site-packages\")\n",
    "\n",
    "print(pyspark.__version__)\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Sedona version: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "sedona_version = pkg_resources.get_distribution(\"apache-sedona\").version\n",
    "print(f\"Apache Sedona version: {sedona_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/latest\n",
      "/usr/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['PYSPARK_PYTHON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://artifacts.unidata.ucar.edu/repository/unidata-all added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /s/chopin/a/grad/flarrieu/.ivy2/cache\n",
      "The jars for the packages stored in: /s/chopin/a/grad/flarrieu/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-3.5_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ca90b1d4-2057-4406-b0ed-337f6a0595cb;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/3.5.0-with-hadoop3.3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.sedona#sedona-spark-3.5_2.12;1.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-common;1.5.1 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.19.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.google.guava#guava;25.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in user-list\n",
      "\tfound org.checkerframework#checker-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound com.uber#h3;4.1.1 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.10.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-spark-common-3.5_2.12;1.5.1 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in user-list\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.5.1-28.2 in central\n",
      ":: resolution report :: resolve 1363ms :: artifacts dl 95ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from user-list in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;25.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.uber#h3;4.1.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from user-list in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-common;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-3.5_2.12;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-common-3.5_2.12;1.5.1 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.5.1-28.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n",
      "\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   23  |   1   |   1   |   2   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ca90b1d4-2057-4406-b0ed-337f6a0595cb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/31ms)\n",
      "24/04/19 16:08:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/tmp/ipykernel_968882/3820514651.py:18: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(spark)\n",
      "/s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages/sedona/register/geo_registrator.py:45: DeprecationWarning: Call to deprecated function register (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  cls.register(spark)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('DatasetIntegration') \\\n",
    "    .master('spark://columbus-oh.cs.colostate.edu:30800') \\\n",
    "    .config(\"spark.yarn.resourcemanager.address\", \"columbia.cs.colostate.edu:30799\") \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "            'org.apache.sedona:sedona-spark-3.5_2.12:1.5.1,'\n",
    "            'org.datasyslab:geotools-wrapper:1.5.1-28.2') \\\n",
    "    .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to DEBUG\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# create a logger\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "logger.info(\"Pyspark initialized...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df: DataFrame):\n",
    "    path = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "    # Here we use mode 'append' to add to the existing file\n",
    "    df.write.csv(path=path, mode='append', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "\n",
    "def get_file_to_df(file_name: str): \n",
    "    data_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/\"\n",
    "    geojsonSchema = \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "    \n",
    "    df = spark.read.schema(geojsonSchema).json(data_directory + file_name, multiLine=True)\n",
    "    \n",
    "    # Explode the features array to create a row for each feature and select the columns\n",
    "    df = (df\n",
    "        .select(F.explode(\"features\").alias(\"features\"))\n",
    "        .select(\"features.*\")\n",
    "        .withColumn(\"geometry\", F.expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "\n",
    "def get_files_recursive(path):\n",
    "    file_status_arr = fs.listStatus(spark._jvm.Path(path))\n",
    "    \n",
    "    file_paths = []\n",
    "    \n",
    "    for file_status in file_status_arr:\n",
    "        if file_status.isDirectory():\n",
    "            file_paths += get_files_recursive(file_status.getPath().toString())\n",
    "        elif file_status.getPath().getName().endswith(('.json', '.geojson')):\n",
    "            file_paths.append(file_status.getPath().toString())\n",
    "    \n",
    "    print(file_paths)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Get Blocks (Leaf Nodes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Alabama.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Alaska.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/AmericanSamoa.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Arizona.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Arkansas.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/California.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Colorado.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/CommonwealthoftheNorthernMarianaIslands.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Connecticut.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Delaware.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/DistrictofColumbia.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Florida.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Georgia.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Guam.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Hawaii.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Idaho.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Illinois.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Indiana.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Iowa.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Kansas.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Kentucky.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Louisiana.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Maine.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Maryland.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Massachusetts.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Michigan.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Minnesota.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Mississippi.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Missouri.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Montana.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Nebraska.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Nevada.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewHampshire.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewJersey.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewMexico.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewYork.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NorthCarolina.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NorthDakota.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Ohio.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Oklahoma.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Oregon.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Pennsylvania.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/PuertoRico.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/RhodeIsland.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/SouthCarolina.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/SouthDakota.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Tennessee.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Texas.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/UnitedStatesVirginIslands.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Utah.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Vermont.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Virginia.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Washington.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/WestVirginia.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Wisconsin.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Wyoming.geojson']\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Alabama.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Alaska.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/AmericanSamoa.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Arizona.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Arkansas.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/California.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Colorado.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/CommonwealthoftheNorthernMarianaIslands.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Connecticut.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Delaware.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/DistrictofColumbia.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Florida.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Georgia.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Guam.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Hawaii.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Idaho.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Illinois.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Indiana.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Iowa.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Kansas.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Kentucky.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Louisiana.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Maine.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Maryland.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Massachusetts.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Michigan.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Minnesota.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Mississippi.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Missouri.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Montana.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Nebraska.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Nevada.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewHampshire.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewJersey.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewMexico.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewYork.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NorthCarolina.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NorthDakota.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Ohio.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Oklahoma.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Oregon.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Pennsylvania.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/PuertoRico.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/RhodeIsland.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/SouthCarolina.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/SouthDakota.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Tennessee.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Texas.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/UnitedStatesVirginIslands.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Utah.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Vermont.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Virginia.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Washington.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/WestVirginia.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Wisconsin.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Wyoming.geojson\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Hadoop file system \n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "\n",
    "blocks_dataframes = {}\n",
    "\n",
    "# Directory containing the files\n",
    "json_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/\"\n",
    "\n",
    "# Define the schema for the GeoJSON data\n",
    "geojsonSchema = \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "files = get_files_recursive(json_directory)\n",
    "# Load each JSON file into a DataFrame and store it in the dictionary\n",
    "for file_path in files:\n",
    "    if file_path:\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        \n",
    "        # Print the file path\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        # Read the GeoJSON file using the defined schema using sedona into a spark dataframe\n",
    "        df = spark.read.schema(geojsonSchema).json(file_path, multiLine=True)\n",
    "        \n",
    "        # Explode the features array to create a row for each feature and select the columns\n",
    "        df = (df\n",
    "            .select(F.explode(\"features\").alias(\"features\"))\n",
    "            .select(\"features.*\")\n",
    "            # Use Sedona's ST_GeomFromGeoJSON function to convert the geometry string to a geometry object\n",
    "            .withColumn(\"geometry\", F.expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "            )\n",
    "        \n",
    "        blocks_dataframes[file_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Get Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: FloodZones.geojson\n"
     ]
    }
   ],
   "source": [
    "df_dataset = get_file_to_df('FloodZones.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Integrate Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|type   |geometry                                                                                                                                                                                                                                                                                                                                                                                                        |properties                                                                                                                                                                                                  |\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Feature|POLYGON ((-86.924385 33.432629, -86.92169 33.43526, -86.91778 33.438672, -86.917696 33.438747, -86.917703 33.438896, -86.912525 33.44319, -86.912092 33.443548, -86.911987 33.443386, -86.909056 33.439261, -86.909982 33.438382, -86.911275 33.437123, -86.913095 33.435119, -86.914149 33.433878, -86.917775 33.43096, -86.91898 33.430042, -86.917939 33.432454, -86.923256 33.432621, -86.924385 33.432629))|{STATEFP -> 01, COUNTYFP -> 073, TRACTCE -> 010500, BLKGRPCE -> 1, AFFGEOID -> 1500000US010730105001, GEOID -> 010730105001, NAME -> 1, NAMELSAD -> Block Group 1, LSAD -> BG, ALAND -> 800598, AWATER -> 0}|\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|type   |geometry                                                                              |properties                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+-------+--------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Feature|LINESTRING (-78.98610698209494 36.3740238349007, -78.98610698209494 36.37395096207564)|{DFIRM_ID -> 37145C, VERSION_ID -> 2.3.3.2, STUDY_TYP -> SFHAs WITH HIGH FLOOD RISK, FLD_ZONE -> AE, ZONE_SUBTY -> NULL, SFHA_TF -> T, STATIC_BFE -> -9999, V_DATUM -> NULL, DEPTH -> -9999, LEN_UNIT -> NULL, VELOCITY -> -9999, VEL_UNIT -> NULL, AR_REVERT -> NULL, AR_SUBTRV -> NULL, BFE_REVERT -> -9999, DEP_REVERT -> -9999, DUAL_ZONE -> NULL, SOURCE_CIT -> 37145C_STUDY84}|\n",
      "+-------+--------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "blocks_dataframes['Alabama.geojson'].show(n=1, truncate=False)\n",
    "df_dataset.show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_dataset(dataset_key: str, df_dataset: DataFrame):\n",
    "    df_dataset = df_dataset.withColumnRenamed(\"geometry\", \"dataset_geometry\")\n",
    "    df_dataset = df_dataset.withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "    for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "        logger.info(f\"Integrating blocks from {blocks_file_name} with dataset {dataset_key}\")\n",
    "        df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\")\n",
    "        df_blocks = df_blocks.withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "        # Using a more appropriate spatial function\n",
    "        df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "            expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "            expr(\"ST_Contains(dataset_geometry, blocks_geometry)\") |\n",
    "            expr(\"ST_Contains(blocks_geometry, dataset_geometry)\") |\n",
    "            expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "            expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\") \n",
    "        )\n",
    "\n",
    "        # Assemble the graph relationships\n",
    "        df_relationships = df_blocks_partOf_dataset.select(\n",
    "            col(f\"dataset_properties.{dataset_key}\").alias(\"Subject\"),\n",
    "            lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "            col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "        )\n",
    "        \n",
    "        #append_to_csv(df_relationships)\n",
    "        return df_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relationships = integrate_dataset('DFIRM_ID', df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/19 16:13:28 ERROR TaskSchedulerImpl: Lost executor 2 on 129.82.44.141: Command exited with code 52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|Subject|Relationship|Object|\n",
      "+-------+------------+------+\n",
      "|01001C |isPartOf    |2     |\n",
      "+-------+------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_relationships.show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6487, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_relationships.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
