{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SedonaKepler import, verify if keplergl is installed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "import pkg_resources\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, lit, expr, broadcast, to_json, explode, split, concat\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum as pyspark_sum\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "import geopandas as gpd\n",
    "from py4j.java_gateway import java_import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n",
      "/usr/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/usr/local/python-env/py39/lib/python3.9/site-packages\")\n",
    "\n",
    "print(pyspark.__version__)\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Sedona version: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "sedona_version = pkg_resources.get_distribution(\"apache-sedona\").version\n",
    "print(f\"Apache Sedona version: {sedona_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/latest\n",
      "/usr/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['PYSPARK_PYTHON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://artifacts.unidata.ucar.edu/repository/unidata-all added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /s/chopin/a/grad/flarrieu/.ivy2/cache\n",
      "The jars for the packages stored in: /s/chopin/a/grad/flarrieu/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-3.5_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bf24faaa-26a5-4785-bd22-610ac3d68eb8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-3.5_2.12;1.5.1 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/3.5.0-with-hadoop3.3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.sedona#sedona-common;1.5.1 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.19.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.google.guava#guava;25.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in user-list\n",
      "\tfound org.checkerframework#checker-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound com.uber#h3;4.1.1 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.10.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-spark-common-3.5_2.12;1.5.1 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in user-list\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.5.1-28.2 in central\n",
      ":: resolution report :: resolve 906ms :: artifacts dl 31ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from user-list in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;25.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.uber#h3;4.1.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from user-list in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-common;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-3.5_2.12;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-common-3.5_2.12;1.5.1 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.5.1-28.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n",
      "\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   23  |   1   |   1   |   2   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bf24faaa-26a5-4785-bd22-610ac3d68eb8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/15ms)\n",
      "24/04/22 23:21:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/tmp/ipykernel_1778913/3616179542.py:22: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(spark)\n",
      "/s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages/sedona/register/geo_registrator.py:45: DeprecationWarning: Call to deprecated function register (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  cls.register(spark)\n"
     ]
    }
   ],
   "source": [
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('DatasetIntegration') \\\n",
    "        .master('spark://columbus-oh.cs.colostate.edu:30800') \\\n",
    "        .config(\"spark.yarn.resourcemanager.address\", \"columbia.cs.colostate.edu:30799\") \\\n",
    "        .config(\"spark.executor.memory\", \"3g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"512m\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"500m\") \\\n",
    "        .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "        .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "        .config('spark.jars.packages',\n",
    "                'org.apache.sedona:sedona-spark-3.5_2.12:1.5.1,'\n",
    "                'org.datasyslab:geotools-wrapper:1.5.1-28.2') \\\n",
    "        .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Set log level to DEBUG\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    sedona = SedonaContext.create(spark)\n",
    "    SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "    # create a logger\n",
    "    logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "    logger.info(\"Pyspark initialized...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(df: DataFrame, path: str):\n",
    "    df.write.csv(path=path, mode='append', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df: DataFrame, path: str):\n",
    "    df.write.csv(path=path, mode='append', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_to_df(file_name: str): \n",
    "    data_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/\"\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "\n",
    "    geojson_schema =  \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "    df = spark.read.schema(geojson_schema).json(data_directory + file_name, multiLine=True)\n",
    "    \n",
    "    # Explode the features array to create a row for each feature and select the columns\n",
    "    df = (df\n",
    "        .select(explode(\"features\").alias(\"features\"))\n",
    "        .select(\"features.*\")\n",
    "        .withColumn(\"geometry\", expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_recursive(path):\n",
    "    file_status_arr = fs.listStatus(spark._jvm.Path(path))\n",
    "    \n",
    "    file_paths = []\n",
    "    \n",
    "    for file_status in file_status_arr:\n",
    "        if file_status.isDirectory():\n",
    "            file_paths += get_files_recursive(file_status.getPath().toString())\n",
    "        elif file_status.getPath().getName().endswith(('.json', '.geojson')):\n",
    "            file_paths.append(file_status.getPath().toString())\n",
    "    \n",
    "    print(file_paths)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Get Blocks (Leaf Nodes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Alabama.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Alaska.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/AmericanSamoa.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Arizona.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Arkansas.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/California.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Colorado.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/CommonwealthoftheNorthernMarianaIslands.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Connecticut.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Delaware.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/DistrictofColumbia.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Florida.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Georgia.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Guam.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Hawaii.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Idaho.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Illinois.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Indiana.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Iowa.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Kansas.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Kentucky.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Louisiana.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Maine.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Maryland.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Massachusetts.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Michigan.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Minnesota.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Mississippi.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Missouri.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Montana.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Nebraska.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Nevada.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewHampshire.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewJersey.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewMexico.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewYork.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NorthCarolina.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NorthDakota.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Ohio.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Oklahoma.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Oregon.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Pennsylvania.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/PuertoRico.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/RhodeIsland.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/SouthCarolina.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/SouthDakota.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Tennessee.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Texas.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/UnitedStatesVirginIslands.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Utah.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Vermont.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Virginia.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Washington.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/WestVirginia.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Wisconsin.geojson', 'hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Wyoming.geojson']\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Alabama.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Alaska.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/AmericanSamoa.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Arizona.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Arkansas.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/California.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Colorado.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/CommonwealthoftheNorthernMarianaIslands.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Connecticut.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Delaware.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/DistrictofColumbia.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Florida.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Georgia.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Guam.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Hawaii.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Idaho.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Illinois.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Indiana.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Iowa.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Kansas.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Kentucky.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Louisiana.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Maine.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Maryland.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Massachusetts.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Michigan.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Minnesota.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Mississippi.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Missouri.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Montana.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Nebraska.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Nevada.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewHampshire.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewJersey.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewMexico.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NewYork.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NorthCarolina.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/NorthDakota.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Ohio.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Oklahoma.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Oregon.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Pennsylvania.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/PuertoRico.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/RhodeIsland.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/SouthCarolina.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/SouthDakota.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Tennessee.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Texas.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/UnitedStatesVirginIslands.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Utah.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Vermont.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Virginia.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Washington.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/WestVirginia.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Wisconsin.geojson\n",
      "Processing file: hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/Wyoming.geojson\n"
     ]
    }
   ],
   "source": [
    "blocks_dataframes = {}\n",
    "\n",
    "# Directory containing the files\n",
    "json_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/\"\n",
    "\n",
    "files = get_files_recursive(json_directory)\n",
    "\n",
    "# Load each JSON file into a DataFrame and store it in the dictionary\n",
    "for file_path in files:\n",
    "    if file_path:\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        \n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        geojsonSchema = \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "        df = spark.read.schema(geojsonSchema).json(file_path, multiLine=True)\n",
    "        \n",
    "        df = (df\n",
    "            .select(explode(\"features\").alias(\"features\"))\n",
    "            .select(\"features.*\")\n",
    "            # Use Sedona's ST_GeomFromGeoJSON function to convert the geometry string to a geometry object\n",
    "            .withColumn(\"geometry\", F.expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "            )\n",
    "        \n",
    "        blocks_dataframes[file_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dataset Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'GeneralManufacturingFacilities'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "dataset_foreign_id = \"properties.NAME\"\n",
    "\n",
    "# \"properties\": {\n",
    "#     \"OBJECTID\": 3,\n",
    "#     \"UNIQUE_ID\": \"N/A\",\n",
    "#     \"NAME\": \"JWS REFRIGERATION & AIR CONDITIONING\",\n",
    "#     \"PHONE\": \"(671) 646-7662\",\n",
    "#     \"FAX\": \"NOT AVAILABLE\",\n",
    "#     \"ADDRESS\": \"290 TUN JOSE SALAS STREET\",\n",
    "#     \"ADDRESS2\": \"SUITE A\",\n",
    "#     \"CITY\": \"TAMUNING\",\n",
    "#     \"STATE\": \"GU\",\n",
    "#     \"ZIP\": \"96913\",\n",
    "#     \"ZIP4\": \"N/A\",\n",
    "#     \"COUNTY\": \"GUAM\",\n",
    "#     \"FIPS\": \"66010\",\n",
    "#     \"MADDRESS\": \"290 TUN JOSE SALAS STREET\",\n",
    "#     \"MCITY\": \"TAMUNING\",\n",
    "#     \"MSTATE\": \"GU\",\n",
    "#     \"MZIP\": \"96913\",\n",
    "#     \"MZIP4\": \"N/A\",\n",
    "#     \"DIRECTIONS\": \"NOT AVAILABLE\",\n",
    "#     \"GEOPREC\": \"BLOCKFACE\",\n",
    "#     \"EMP\": 0,\n",
    "#     \"PRODUCT\": \"REFRIGERATION AND AIR-CONDITIONING\",\n",
    "#     \"SIC\": \"3585\",\n",
    "#     \"SIC2\": \"2542\",\n",
    "#     \"SIC3\": \"N/A\",\n",
    "#     \"SIC4\": \"N/A\",\n",
    "#     \"NAICS\": \"N/A\",\n",
    "#     \"NAICSDESCR\": \"NOT AVAILABLE\",\n",
    "#     \"WEB\": \"WWW.JWSGUAM.COM\",\n",
    "#     \"LONGITUDE\": 144.7883065,\n",
    "#     \"LATITUDE\": 13.4912295\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"GeneralManufacturingFacility\")) \\\n",
    "                 .withColumn(\"Name\", col(\"properties.NAME\")) \\\n",
    "                 .withColumn(\"Phone\", col(\"properties.PHONE\")) \\\n",
    "                 .withColumn(\"Fax\", col(\"properties.FAX\")) \\\n",
    "                 .withColumn(\"Address\", col(\"properties.ADDRESS\")) \\\n",
    "                 .withColumn(\"City\", col(\"properties.CITY\")) \\\n",
    "                 .withColumn(\"State\", col(\"properties.STATE\")) \\\n",
    "                 .withColumn(\"Zip\", col(\"properties.ZIP\")) \\\n",
    "                 .withColumn(\"County\", col(\"properties.COUNTY\"))\n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Empty Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate Nodes in Base Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HydroCarbonGasLiquidPipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: HydroCarbonGasLiquidPipelines.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|                Type|             Node_ID|           Opername|            Pipename|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|HydroCarbonGasLiq...|HydroCarbonGasLiq...|    ENERGY TRANSFER| West Texas Pipeline|\n",
      "|HydroCarbonGasLiq...|HydroCarbonGasLiq...|ENTERPRISE PRODUCTS|Hobbs East Gathering|\n",
      "|HydroCarbonGasLiq...|HydroCarbonGasLiq...|      NUSTAR ENERGY|      LPG to Reynosa|\n",
      "|HydroCarbonGasLiq...|HydroCarbonGasLiq...|ENTERPRISE PRODUCTS|                ATEX|\n",
      "|HydroCarbonGasLiq...|HydroCarbonGasLiq...|ENTERPRISE PRODUCTS|East Leg - East Loop|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset = 'HydroCarbonGasLiquidPipelines'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "# \"properties\": {\n",
    "#     \"FID\": 28,\n",
    "#     \"Opername\": \"ENERGY TRANSFER\",\n",
    "#     \"Pipename\": \"West Texas Pipeline\",\n",
    "#     \"Shape_Leng\": 9.96270805248,\n",
    "#     \"Shape__Length\": 1148322.03802495\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.FID\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"HydroCarbonGasLiquidPipelines\")) \\\n",
    "                 .withColumn(\"Opername\", col(\"properties.Opername\")) \\\n",
    "                 .withColumn(\"Pipename\", col(\"properties.Pipename\")) \n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Refineries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: OilRefineries.geojson\n",
      "+-------------+--------------------+--------+\n",
      "|         Type|             Node_ID|CAPACITY|\n",
      "+-------------+--------------------+--------+\n",
      "|OilRefineries|OilRefineries_LAK...|  425000|\n",
      "|OilRefineries|OilRefineries_PAS...|  112229|\n",
      "|OilRefineries|OilRefineries_TUL...|   70300|\n",
      "|OilRefineries|OilRefineries_CHA...|  190000|\n",
      "|OilRefineries|OilRefineries_WIL...|  139000|\n",
      "+-------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'OilRefineries'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "# \"properties\": {\n",
    "#     \"OBJECTID\": 18,\n",
    "#     \"REF_ID\": \"REF220020\",\n",
    "#     \"NAME\": \"LAKE CHARLES\",\n",
    "#     \"ADDRESS\": \"1601 HWY 108 E\",\n",
    "#     \"CITY\": \"SULPHUR\",\n",
    "#     \"STATE\": \"LA\",\n",
    "#     \"ZIP\": \"70665\",\n",
    "#     \"ZIP4\": \"NOT AVAILABLE\",\n",
    "#     \"TELEPHONE\": \"(337) 708-8431\",\n",
    "#     \"TYPE\": \"MODERN DEEP-CONVERSION FACILITY\",\n",
    "#     \"STATUS\": \"IN SERVICE\",\n",
    "#     \"POPULATION\": 1183,\n",
    "#     \"COUNTY\": \"CALCASIEU\",\n",
    "#     \"COUNTYFIPS\": \"22019\",\n",
    "#     \"COUNTRY\": \"USA\",\n",
    "#     \"LATITUDE\": 30.17866697000005,\n",
    "#     \"LONGITUDE\": -93.33023517799995,\n",
    "#     \"NAICS_CODE\": \"324110\",\n",
    "#     \"NAICS_DESC\": \"PETROLEUM REFINERIES\",\n",
    "#     \"SOURCE\": \"EIA-820; EPA TRI\",\n",
    "#     \"SOURCEDATE\": \"2017/01/01 00:00:00\",\n",
    "#     \"VAL_METHOD\": \"IMAGERY/OTHER\",\n",
    "#     \"VAL_DATE\": \"2018/01/31 00:00:00\",\n",
    "#     \"WEBSITE\": \"http://citgorefining.com\",\n",
    "#     \"OWNER\": \"PDV AMERICA INC\",\n",
    "#     \"OPERNAME\": \"CITGO PETROLEUM CORP\",\n",
    "#     \"RMP_ID\": \"55717\",\n",
    "#     \"EPA_ID\": \"100000140199\",\n",
    "#     \"POSREL\": \"WITHIN 166 FEET\",\n",
    "#     \"CAPACITY\": 425000,\n",
    "#     \"US_RANK\": 6,\n",
    "#     \"CRUDE\": 425000,\n",
    "#     \"VACDIST\": 230000,\n",
    "#     \"COKING\": 85410,\n",
    "#     \"THERMALOP\": 0,\n",
    "#     \"CATCRACK\": 143000,\n",
    "#     \"CATREFORM\": 103035,\n",
    "#     \"CATHYDCRCK\": 46000,\n",
    "#     \"CATHYDTRT\": 398200,\n",
    "#     \"ALKY\": 26400,\n",
    "#     \"POLDIM\": 0,\n",
    "#     \"AROMATIC\": 20900,\n",
    "#     \"ISOMER\": 28000,\n",
    "#     \"LUBES\": 0,\n",
    "#     \"OXYGENATES\": 0,\n",
    "#     \"HYDRGN\": 0,\n",
    "#     \"COKE\": 32820,\n",
    "#     \"SULFUR\": 717,\n",
    "#     \"ASPHALT\": 0\n",
    "# }\n",
    "\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"OilRefineries\")) \\\n",
    "                 .withColumn(\"CAPACITY\", col(\"properties.CAPACITY\")) \n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaturalGasStorageFacilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: NaturalGasStorageFacilities.geojson\n",
      "+-------------+--------------------+--------------+\n",
      "|         Type|             Node_ID|Total Capacity|\n",
      "+-------------+--------------------+--------------+\n",
      "|OilRefineries|NaturalGasStorage...|      86000000|\n",
      "|OilRefineries|NaturalGasStorage...|      54400193|\n",
      "|OilRefineries|NaturalGasStorage...|        320340|\n",
      "|OilRefineries|NaturalGasStorage...|        654231|\n",
      "|OilRefineries|NaturalGasStorage...|      46854000|\n",
      "+-------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'NaturalGasStorageFacilities'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "# \"properties\": {\n",
    "#     \"FID\": 12,\n",
    "#     \"STFID\": \"STF190012\",\n",
    "#     \"NAME\": \"COLUMBUS CITY\",\n",
    "#     \"ADDRESS\": \"120TH STREET/S AVE\",\n",
    "#     \"CITY\": \"COLUMBUS CITY\",\n",
    "#     \"STATE\": \"IA\",\n",
    "#     \"ZIP\": \"52738\",\n",
    "#     \"ZIP4\": \"NOT AVAILABLE\",\n",
    "#     \"TELEPHONE\": \"NOT AVAILABLE\",\n",
    "#     \"TYPE\": \"AQUIFER\",\n",
    "#     \"STATUS\": \"ACTIVE\",\n",
    "#     \"POPULATION\": -999,\n",
    "#     \"COUNTY\": \"LOUISA\",\n",
    "#     \"COUNTYFIPS\": \"19115\",\n",
    "#     \"COUNTRY\": \"USA\",\n",
    "#     \"LATITUDE\": 41.234864,\n",
    "#     \"LONGITUDE\": -91.350155,\n",
    "#     \"NAICS_CODE\": \"486210\",\n",
    "#     \"NAICS_DESC\": \"STORAGE OF NATURAL GAS\",\n",
    "#     \"SOURCE\": \"EIA, IMAGERY\",\n",
    "#     \"SOURCEDATE\": \"2018/12/01 00:00:00\",\n",
    "#     \"VAL_METHOD\": \"IMAGERY/OTHER\",\n",
    "#     \"VAL_DATE\": \"2019/04/10 00:00:00\",\n",
    "#     \"WEBSITE\": \"http://www.kindermorgan.com/\",\n",
    "#     \"EPAID\": \"155321\",\n",
    "#     \"OWNER\": \"KINDER MORGAN (NATURAL GAS PIPELINE CO OF AMERICA)\",\n",
    "#     \"OPERATOR\": \"NATURAL GAS PIPELINE CO OF AMERICA\",\n",
    "#     \"POSREL\": \"EXCEEDS 1 MILE\",\n",
    "#     \"OWNERPCT\": 100,\n",
    "#     \"MAXDEL\": 175000,\n",
    "#     \"WORKCAP\": 16685000,\n",
    "#     \"BASEGAS\": 37700000,\n",
    "#     \"TOTALCAP\": 54400193,\n",
    "#     \"REGION\": \"MIDWEST REGION\",\n",
    "#     \"PROPMAX\": -999,\n",
    "#     \"PROPWORK\": -999,\n",
    "#     \"PROPTOTAL\": -999,\n",
    "#     \"RESERVNAME\": \"GALESVILLE MT. SIMON ST. PETER\",\n",
    "#     \"SEC_NAICS\": \"NOT APPLICABLE\",\n",
    "#     \"SEC_N_DESC\": \"NOT APPLICABLE\"\n",
    "# }\n",
    "\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"NaturalGasStorageFacilities\")) \\\n",
    "                 .withColumn(\"Total Capacity\", col(\"properties.TOTALCAP\")) \n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaturalGasProcessingPlants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: NaturalGasProcessingPlants.geojson\n",
      "+-------------+--------------------+----------+\n",
      "|         Type|             Node_ID|Plant Flow|\n",
      "+-------------+--------------------+----------+\n",
      "|OilRefineries|NaturalGasProcess...|         6|\n",
      "|OilRefineries|NaturalGasProcess...|         4|\n",
      "|OilRefineries|NaturalGasProcess...|      -999|\n",
      "|OilRefineries|NaturalGasProcess...|        11|\n",
      "|OilRefineries|NaturalGasProcess...|        11|\n",
      "+-------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'NaturalGasProcessingPlants'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "# \"properties\": {\n",
    "#     \"OBJECTID\": 5,\n",
    "#     \"NGPPID\": \"NGPP010177\",\n",
    "#     \"NAME\": \"DOGWOOD OAKS PLANT\",\n",
    "#     \"ADDRESS\": \"21680 HWY 41\",\n",
    "#     \"CITY\": \"BREWTON\",\n",
    "#     \"STATE\": \"AL\",\n",
    "#     \"ZIP\": \"36426\",\n",
    "#     \"ZIP4\": \"NOT AVAILABLE\",\n",
    "#     \"TELEPHONE\": \"(251) 248-2903\",\n",
    "#     \"TYPE\": \"NATURAL GAS PROCESSING PLANT\",\n",
    "#     \"STATUS\": \"ACTIVE\",\n",
    "#     \"POPULATION\": 12,\n",
    "#     \"COUNTY\": \"ESCAMBIA\",\n",
    "#     \"COUNTYFIPS\": \"01053\",\n",
    "#     \"COUNTRY\": \"USA\",\n",
    "#     \"LATITUDE\": 31.243471,\n",
    "#     \"LONGITUDE\": -87.187836,\n",
    "#     \"NAICS_CODE\": \"211130\",\n",
    "#     \"NAICS_DESC\": \"NATURAL GAS EXTRACTION\",\n",
    "#     \"SOURCE\": \"EPA RISK MANAGEMENT PLAN (RMP) - THE RIGHT-TO-KNOW NETWORK\",\n",
    "#     \"SOURCEDATE\": \"2013/03/22 00:00:00\",\n",
    "#     \"VAL_METHOD\": \"IMAGERY/OTHER\",\n",
    "#     \"VAL_DATE\": \"2015/06/17 00:00:00\",\n",
    "#     \"WEBSITE\": \"www.plainsallamerican.com/\",\n",
    "#     \"FACID\": \"100000218356\",\n",
    "#     \"COMPNAME\": \"PLAINS GAS SOLUTIONS, LLC\",\n",
    "#     \"POSREL\": \"WITHIN 40 FEET\",\n",
    "#     \"OPERATOR\": \"CDM MAX, L.L.C. (PLAINS GAS SOLUTIONS, LLC)\",\n",
    "#     \"OPERADDR\": \"333 CLAY STREET, SUITE 1600\",\n",
    "#     \"OPERCITY\": \"HOUSTON\",\n",
    "#     \"OPERSTATE\": \"TX\",\n",
    "#     \"OPERCNTRY\": \"USA\",\n",
    "#     \"OPERZIP\": \"77002\",\n",
    "#     \"OPERPHONE\": \"(251) 248-2903\",\n",
    "#     \"OPERURL\": \"www.plainsallamerican.com/about-us/subsidiary-websites/plains-gas-solutions/facilities\",\n",
    "#     \"GASCAP\": 4,\n",
    "#     \"PROCAMTBLS\": 186840,\n",
    "#     \"BASIN\": \"GULF COAST COAL REGION\",\n",
    "#     \"PLANTFLOW\": 4,\n",
    "#     \"BTUCONTENT\": 1000,\n",
    "#     \"GASSTORCAP\": -999,\n",
    "#     \"LIQSTORCAP\": 1000,\n",
    "#     \"RMP_ID\": \"1000032802\",\n",
    "#     \"EPA_ID\": \"110055375883\"\n",
    "# }\n",
    "\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"NaturalGasProcessingPlants\")) \\\n",
    "                 .withColumn(\"Plant Flow\", col(\"properties.PLANTFLOW\")) \n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaturalGasCompressorStations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GeographicRegions.geojson\n",
      "+-----------------+--------------------+----------+\n",
      "|             Type|             Node_ID|Scale Rank|\n",
      "+-----------------+--------------------+----------+\n",
      "|GeographicRegions|GeographicRegions...|       7.0|\n",
      "|GeographicRegions|GeographicRegions...|       7.0|\n",
      "|GeographicRegions|GeographicRegions...|       7.0|\n",
      "|GeographicRegions|GeographicRegions...|       7.0|\n",
      "|GeographicRegions|GeographicRegions...|       7.0|\n",
      "+-----------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'GeographicRegions'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "# \"properties\": { \n",
    "#     \"scalerank\": 7.0, \n",
    "#     \"featurecla\": \"Island\", \n",
    "#     \"name\": \"Adak\", \n",
    "#     \"namealt\": null, \n",
    "#     \"region\": \"North America\", \n",
    "#     \"subregion\": null \n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.name\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"GeographicRegions\")) \\\n",
    "                 .withColumn(\"Scale Rank\", col(\"properties.scalerank\")) \n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 542:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.name\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.name\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PowerPlants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'PowerPlants'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "# \"properties\": {\n",
    "#     \"PGM_SYS_AC\": \"EIA-860\",\n",
    "#     \"PGM_SYS_ID\": \"124\",\n",
    "#     \"REGISTRY_I\": \"110002569569\",\n",
    "#     \"PRIMARY_NA\": \"TUCSON ELECTRIC POWER DEMOSS-PETRIE DSL\",\n",
    "#     \"LOCATION_A\": \"2501 NORTH FLOWING WELLS ROAD\",\n",
    "#     \"CITY_NAME\": \"TUCSON\",\n",
    "#     \"COUNTY_NAM\": \"PIMA\",\n",
    "#     \"STATE_CODE\": \"AZ\",\n",
    "#     \"POSTAL_COD\": \"85705-4015\",\n",
    "#     \"FEDERAL_FA\": \"N\",\n",
    "#     \"TRIBAL_LAN\": \"\",\n",
    "#     \"DATA_QUALI\": \"V\",\n",
    "#     \"LAST_REPOR\": \"\",\n",
    "#     \"CREATE_DAT\": \"2000-03-01\",\n",
    "#     \"UPDATE_DAT\": \"2014-04-30\",\n",
    "#     \"LATITUDE83\": 32.2523193359375,\n",
    "#     \"LONGITUDE8\": -110.99149322509766,\n",
    "#     \"REF_POINT_\": \"CENTER OF A FACILITY OR STATION\",\n",
    "#     \"DERIVED_HU\": \"15050301\",\n",
    "#     \"DERIVED_WB\": \"150503010906\",\n",
    "#     \"DERIVED_CB\": \"040190045044008\",\n",
    "#     \"DERIVED_CD\": \"02\",\n",
    "#     \"OZONE_8HR_\": \"\",\n",
    "#     \"PB_2008_AR\": \"\",\n",
    "#     \"PM25_1997_\": \"\",\n",
    "#     \"PM25_2006_\": \"\",\n",
    "#     \"OZONE_8H_1\": \"\",\n",
    "#     \"UTILITY_ID\": \"24211\",\n",
    "#     \"UTILITY_NA\": \"Tucson Electric Power Co\",\n",
    "#     \"PLANT_CODE\": \"124\",\n",
    "#     \"PLANT_NAME\": \"Demoss Petrie\",\n",
    "#     \"GENERATOR_\": \"GT2\",\n",
    "#     \"PRIME_MOVE\": \"GT\",\n",
    "#     \"STATUS\": \"OP\",\n",
    "#     \"NAMEPLATE\": 85,\n",
    "#     \"SUMMER_CAP\": 72.19999694824219,\n",
    "#     \"WINTER_CAP\": 83.30000305175781,\n",
    "#     \"UNIT_CODE\": \"\",\n",
    "#     \"OPERATING_\": \"6\",\n",
    "#     \"OPERATIN_1\": \"2001\",\n",
    "#     \"ENERGY_SOU\": \"NG\",\n",
    "#     \"ENERGY_S_1\": \"\",\n",
    "#     \"ENERGY_S_2\": \"\",\n",
    "#     \"ENERGY_S_3\": \"\",\n",
    "#     \"ENERGY_S_4\": \"\",\n",
    "#     \"ENERGY_S_5\": \"\",\n",
    "#     \"MULTIPLE_F\": \"N\",\n",
    "#     \"DELIVER_PO\": \"Y\",\n",
    "#     \"SYNCHRONIZ\": \"\",\n",
    "#     \"OWNERSHIP\": \"S\",\n",
    "#     \"TURBINES\": \".\",\n",
    "#     \"COGENERATO\": \"N\",\n",
    "#     \"SECTOR_NAM\": \"Electric Utility\",\n",
    "#     \"SECTOR\": \"1\",\n",
    "#     \"TOPPING_BO\": \"\",\n",
    "#     \"DUCT_BURNE\": \"N\",\n",
    "#     \"PLANNED_MO\": \"N\",\n",
    "#     \"PLANNED_UP\": \".\",\n",
    "#     \"PLANNED__1\": \".\",\n",
    "#     \"PLANNED__2\": \".\",\n",
    "#     \"PLANNED__3\": \".\",\n",
    "#     \"PLANNED_DE\": \".\",\n",
    "#     \"PLANNED__4\": \".\",\n",
    "#     \"PLANNED__5\": \".\",\n",
    "#     \"PLANNED__6\": \".\",\n",
    "#     \"PLANNED_NE\": \"\",\n",
    "#     \"PLANNED_EN\": \"\",\n",
    "#     \"PLANNED_RE\": \".\",\n",
    "#     \"PLANNED__7\": \".\",\n",
    "#     \"OTHER_MODS\": \"\",\n",
    "#     \"OTHER_MOD_\": \".\",\n",
    "#     \"OTHER_MO_1\": \".\",\n",
    "#     \"PLANNED__8\": \".\",\n",
    "#     \"PLANNED__9\": \".\",\n",
    "#     \"SFG_SYSTEM\": \"N\",\n",
    "#     \"PULVERIZED\": \"\",\n",
    "#     \"FLUIDIZED_\": \"\",\n",
    "#     \"SUBCRITICA\": \"\",\n",
    "#     \"SUPERCRITI\": \"\",\n",
    "#     \"ULTRASUPER\": \"\",\n",
    "#     \"CARBONCAPT\": \"\",\n",
    "#     \"STARTUP_SO\": \"\",\n",
    "#     \"STARTUP__1\": \"\",\n",
    "#     \"STARTUP__2\": \"\",\n",
    "#     \"STARTUP__3\": \"\",\n",
    "#     \"ENERGY_SRC\": \"Natural Gas\",\n",
    "#     \"ENERGY_S_6\": \"\"\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.PRIMARY_NA\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"PowerPlants\")) \\\n",
    "                 .withColumn(\"Summer Capacity\", col(\"properties.SUMMER_CAP\")) \\\n",
    "                .withColumn(\"Winter Capacity\", col(\"properties.WINTER_CAP\")) \n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.PRIMARY_NA\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.PRIMARY_NA\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaturalGasPipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'NaturalGasPipelines'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "# \"properties\": {\n",
    "#     \"TYPEPIPE\": \"Intrastate\",\n",
    "#     \"Operator\": \"Crosstex Texas Systems\",\n",
    "#     \"Shape_Leng\": 0.00187974387,\n",
    "#     \"Shape__Len\": 240.3441469695\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.Operator\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"NaturalGasPipelines\")) \\\n",
    "                 .withColumn(\"Pipe Type\", col(\"properties.TYPEPIPE\")) \n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.Operator\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.Operator\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Dams'\n",
    "\n",
    "node_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Nodes.csv\"\n",
    "edge_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/output/{dataset}Edges.csv\"\n",
    "\n",
    "data_directory = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/{dataset}.geojson\"\n",
    "\n",
    "df = get_file_to_df(f'{dataset}.geojson')\n",
    "\n",
    "#  \"properties\": {\n",
    "#     \"OBJECTID\": 9144,\n",
    "#     \"NGAID\": \"10130292\",\n",
    "#     \"METLNKID\": \"\",\n",
    "#     \"FEATTYPE\": \"POLYLINE\",\n",
    "#     \"NAME\": \"EL PASO DAM NO 10\",\n",
    "#     \"CITY\": \"EL PASO\",\n",
    "#     \"STATE\": \"TX\",\n",
    "#     \"COUNTY\": \"EL PASO\",\n",
    "#     \"FIPS\": \"48141\",\n",
    "#     \"DIRECTIONS\": \"\",\n",
    "#     \"EMERGTITLE\": \"\",\n",
    "#     \"EMERGPHONE\": \"\",\n",
    "#     \"EMERGEXT\": \"\",\n",
    "#     \"CONTDATE\": \"1899-11-30T00:00:00.000Z\",\n",
    "#     \"CONTHOW\": \"\",\n",
    "#     \"GEODATE\": \"2007-03-28T00:00:00.000Z\",\n",
    "#     \"GEOHOW\": \"MANUAL\",\n",
    "#     \"HSIPTHEMES\": \"CRITICAL INFRASTUCTURE, PDD-63; WATER SUPPLY; DAMS\",\n",
    "#     \"SOURCE\": \"USACE\",\n",
    "#     \"X\": -106.4814352,\n",
    "#     \"Y\": 31.7778363,\n",
    "#     \"QC_QA\": \"\",\n",
    "#     \"RECORDID\": \"72827\",\n",
    "#     \"OTHER_NAME\": \"\",\n",
    "#     \"FORM_NAME\": \"\",\n",
    "#     \"STATEID\": \"\",\n",
    "#     \"NIDID\": \"TX07023\",\n",
    "#     \"SECTION\": \"3106-432\",\n",
    "#     \"RIVER\": \"OFF CH-RIO GRANDE\",\n",
    "#     \"CITYAFFECT\": \"EL PASO\",\n",
    "#     \"NIDSTATE\": \"TX\",\n",
    "#     \"NIDCOUNTY\": \"EL PASO\",\n",
    "#     \"DISTANCE\": 0,\n",
    "#     \"OWN_TYPE\": \"L\",\n",
    "#     \"PRIV_DAM\": \"\",\n",
    "#     \"DAM_TYPE\": \"RE\",\n",
    "#     \"CORE\": \"XX\",\n",
    "#     \"FOUND\": \"U\",\n",
    "#     \"PURPOSES\": \"C\",\n",
    "#     \"YR_COMPL\": \"\",\n",
    "#     \"YR_MOD\": \"\",\n",
    "#     \"DAM_LENGTH\": 0,\n",
    "#     \"DAM_HEIGHT\": 30,\n",
    "#     \"STR_HEIGHT\": 0,\n",
    "#     \"HYD_HEIGHT\": 0,\n",
    "#     \"NID_HEIGHT\": 30,\n",
    "#     \"MAX_DIS\": 0,\n",
    "#     \"MAX_STOR\": 24,\n",
    "#     \"NORMAL_STO\": 0,\n",
    "#     \"NID_STOR\": 24,\n",
    "#     \"SURF_AREA\": 0,\n",
    "#     \"DRAIN_AREA\": 0,\n",
    "#     \"HAZARD\": \"H\",\n",
    "#     \"EAP\": \"N\",\n",
    "#     \"INSP_DATE\": \"1996-07-17T00:00:00.000Z\",\n",
    "#     \"INSP_FREQU\": 0,\n",
    "#     \"ST_REG_DAM\": \"Y\",\n",
    "#     \"ST_REG_AG\": \"\",\n",
    "#     \"SPILL_TYPE\": \"U\",\n",
    "#     \"SPILL_WIDT\": 12,\n",
    "#     \"OUT_GATES\": \"\",\n",
    "#     \"VOLUME\": 0,\n",
    "#     \"NO_LOCKS\": 0,\n",
    "#     \"LEN_LOCKS\": 0,\n",
    "#     \"WID_LOCKS\": 0,\n",
    "#     \"FED_FUND\": \"\",\n",
    "#     \"FED_DESIGN\": \"\",\n",
    "#     \"FED_CONSTR\": \"\",\n",
    "#     \"FED_REG\": \"\",\n",
    "#     \"FED_INSP\": \"\",\n",
    "#     \"FED_OPER\": \"\",\n",
    "#     \"FED_OWN\": \"\",\n",
    "#     \"FED_OTHER\": \"\",\n",
    "#     \"SOURCE_A\": \"TX\",\n",
    "#     \"SUB_DATE\": \"20000401\",\n",
    "#     \"URL_ADDRES\": \"HTTP://WWW.TCEQ.STATE.TX.US/\",\n",
    "#     \"CONG_DIST\": \"TX16\",\n",
    "#     \"SHAPE_Leng\": 216.77816378547902\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"Dams\")) \\\n",
    "                 .withColumn(\"River\", col(\"properties.RIVER\")) \n",
    "\n",
    "df_nodes = df_nodes.drop(\"geometry\", \"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n",
    "\n",
    "create_csv(df_nodes, node_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Subject\", StringType(), True),\n",
    "    StructField(\"Relationship\", StringType(), True),\n",
    "    StructField(\"Object\", StringType(), True)\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "empty_df.write.csv(path=edge_path, mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_graph_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "\n",
    "df_dataset = df.withColumnRenamed(\"geometry\", \"dataset_geometry\") \\\n",
    "                    .withColumnRenamed(\"properties\", \"dataset_properties\")\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    df_blocks = df_blocks.withColumnRenamed(\"geometry\", \"blocks_geometry\") \\\n",
    "                            .withColumnRenamed(\"properties\", \"blocks_properties\")\n",
    "\n",
    "    df_blocks_partOf_dataset = df_blocks.crossJoin(df_dataset).where(\n",
    "        expr(\"ST_Intersects(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Contains(blocks_geometry, ST_Centroid(dataset_geometry))\") |\n",
    "        expr(\"ST_Touches(dataset_geometry, blocks_geometry)\") |\n",
    "        expr(\"ST_Overlaps(dataset_geometry, blocks_geometry)\")\n",
    "    )\n",
    "\n",
    "    dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "    df_isPartOf_relationships = df_blocks_partOf_dataset.select(\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Subject\"),\n",
    "        lit(\"isPartOf\").alias(\"Relationship\"),\n",
    "        col(\"blocks_properties.NAME\").alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    df_contains_relationships = df_blocks_partOf_dataset.select(\n",
    "        col(\"blocks_properties.NAME\").alias(\"Subject\"),\n",
    "        lit(\"Contains\").alias(\"Relationship\"),\n",
    "        concat(dataset_prefix, col(\"dataset_properties.NAME\")).alias(\"Object\")\n",
    "    )\n",
    "\n",
    "    # Update the Edges.csv\n",
    "    append_to_csv(df_isPartOf_relationships, edge_path)\n",
    "    # Update the base grap\n",
    "    # append_to_csv(df_contains_relationships, base_graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
