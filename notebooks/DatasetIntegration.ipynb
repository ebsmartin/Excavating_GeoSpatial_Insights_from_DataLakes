{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/local/python-env/py39/lib/python3.9/site-packages\")\n",
    "\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialze a SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring the pyspark library is being accessed from my local usr directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "sedona_version = pkg_resources.get_distribution(\"apache-sedona\").version\n",
    "print(f\"Apache Sedona version: {sedona_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['PYSPARK_PYTHON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, lit,  split, expr, concat, when, explode\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.functions import year \n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum as pyspark_sum\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to make the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('GeoSpatialQueries_Freddy') \\\n",
    "    .master('spark://columbus-oh.cs.colostate.edu:30800') \\\n",
    "    .config(\"spark.yarn.resourcemanager.address\", \"columbia.cs.colostate.edu:30799\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"500m\") \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "            'org.apache.sedona:sedona-spark-3.5_2.12:1.5.1,'\n",
    "            'org.datasyslab:geotools-wrapper:1.5.1-28.2') \\\n",
    "    .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to DEBUG\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# create a logger\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "logger.info(\"Pyspark initialized...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module from py4j to interact with JVM\n",
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Import the Path class from Hadoop. This class is used to handle file paths in Hadoop.\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "\n",
    "def get_files_recursive(path):\n",
    "    file_status_arr = fs.listStatus(spark._jvm.Path(path))\n",
    "    file_paths = []\n",
    "    \n",
    "    for file_status in file_status_arr:\n",
    "        if file_status.isDirectory():\n",
    "            file_paths += get_files_recursive(file_status.getPath().toString())\n",
    "        elif file_status.getPath().getName().endswith(('.geojson')):\n",
    "            file_paths.append(file_status.getPath().toString())\n",
    "    \n",
    "    print(file_paths)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(df: DataFrame, path: str):\n",
    "    df.write.csv(path=path, mode='append', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df: DataFrame, path: str):\n",
    "    df.write.csv(path=path, mode='append', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_to_df(file_name: str): \n",
    "    data_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/\"\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "\n",
    "    geojson_schema =  \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "    df = spark.read.schema(geojson_schema).json(data_directory + file_name, multiLine=True)\n",
    "    \n",
    "    df = (df\n",
    "        .select(explode(\"features\").alias(\"features\"))\n",
    "        .select(\"features.*\")\n",
    "        .withColumn(\"geometry\", expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_df(df_parent: DataFrame, subject, relationship, object):\n",
    "    df_edges = df_parent.withColumn(\"Subject\", subject) \\\n",
    "        .withColumn(\"Relationship\", relationship) \\\n",
    "        .withColumn(\"Object\", object) \\\n",
    "        .select(\"Subject\", \"Relationship\", \"Object\")\n",
    "    return df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node_df(df_edges: DataFrame, type):\n",
    "    nodes_subject = df_edges.select(col(\"Subject\").alias(\"Node_ID\")).distinct()\n",
    "    nodes_object = df_edges.select(col(\"Object\").alias(\"Node_ID\")).distinct()\n",
    "\n",
    "    nodes_df = nodes_subject.union(nodes_object).distinct()\n",
    "\n",
    "    nodes_df = nodes_df.withColumn(\"Type\", type)\n",
    "    \n",
    "    return nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relationships_point(df_parent: DataFrame, df_child: DataFrame, ):\n",
    "    df_parent = df_parent.withColumnRenamed(\"geometry\", \"parent_geometry\")\n",
    "    df_child = df_child.withColumnRenamed(\"geometry\", \"child_geometry\")\n",
    "    df_parent = df_parent.withColumnRenamed(\"properties\", \"parent_properties\")\n",
    "    df_child = df_child.withColumnRenamed(\"properties\", \"child_properties\")\n",
    "\n",
    "    df_parent.createOrReplaceTempView(\"parents\")\n",
    "    df_child.createOrReplaceTempView(\"children\")\n",
    "\n",
    "    df_country_continent_contains = spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM parents, children\n",
    "        WHERE ST_Contains(parent_geometry, ST_Centroid(child_geometry))\n",
    "    \"\"\")\n",
    "\n",
    "    return df_country_continent_contains.select(\"parent_geometry\", \"child_geometry\", \"parent_properties\", \"child_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relationships_geometry(df_parent: DataFrame, df_child: DataFrame, ):\n",
    "    df_parent = df_parent.withColumnRenamed(\"geometry\", \"parent_geometry\")\n",
    "    df_child = df_child.withColumnRenamed(\"geometry\", \"child_geometry\")\n",
    "    df_parent = df_parent.withColumnRenamed(\"properties\", \"parent_properties\")\n",
    "    df_child = df_child.withColumnRenamed(\"properties\", \"child_properties\")\n",
    "\n",
    "    df_parent.createOrReplaceTempView(\"parents\")\n",
    "    df_child.createOrReplaceTempView(\"children\")\n",
    "\n",
    "    df_country_continent_contains = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            p.parent_geometry, \n",
    "            c.child_geometry, \n",
    "            p.parent_properties, \n",
    "            c.child_properties,\n",
    "            CASE \n",
    "                WHEN ST_Contains(p.parent_geometry, ST_Centroid(c.child_geometry)) THEN 'Contains'\n",
    "                WHEN ST_DWithin(p.parent_geometry, ST_Centroid(c.child_geometry), 100) THEN 'Within 100m'\n",
    "                WHEN ST_Touches(p.parent_geometry, c.child_geometry) THEN 'Touches'\n",
    "                WHEN ST_Intersects(p.parent_geometry, c.child_geometry) THEN 'Intersects'\n",
    "            END AS relationship_type\n",
    "        FROM parents p, children c\n",
    "        WHERE ST_Contains(p.parent_geometry, ST_Centroid(c.child_geometry))\n",
    "        OR ST_DWithin(p.parent_geometry, ST_Centroid(c.child_geometry), 100)\n",
    "        OR ST_Touches(p.parent_geometry, c.child_geometry)\n",
    "        OR ST_Intersects(p.parent_geometry, c.child_geometry)\n",
    "    \"\"\")\n",
    "\n",
    "    return df_country_continent_contains.select(\"parent_geometry\", \"child_geometry\", \"parent_properties\", \"child_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "def union_all(dfs_dict):\n",
    "    dfs = list(dfs_dict.values())\n",
    "    if dfs:\n",
    "        return reduce(DataFrame.unionByName, dfs)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_edges_path = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/BaseEdges.csv\"\n",
    "base_nodes_path = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/BaseNodes.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Get Blocks (Leaf Nodes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_dataframes = {}\n",
    "files_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/BlocksByState/\"\n",
    "\n",
    "files = get_files_recursive(files_directory)\n",
    "\n",
    "for file_path in files:\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    \n",
    "    print(f\"Processing file: {file_path}\")\n",
    "\n",
    "    geojsonSchema = \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "    \n",
    "    df = spark.read.schema(geojsonSchema).json(file_path, multiLine=True)\n",
    "    \n",
    "    df = (df\n",
    "        .select(F.explode(\"features\").alias(\"features\"))\n",
    "        .select(\"features.*\")\n",
    "        .withColumn(\"geometry\", F.expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "        )\n",
    "    \n",
    "    blocks_dataframes[file_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get County Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_dataframes = {}\n",
    "files_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/CountiesByState/\"\n",
    "files = get_files_recursive(files_directory)\n",
    "\n",
    "for file_path in files:\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    \n",
    "    # Print the file path\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "\n",
    "    geojsonSchema = \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "    df = spark.read.schema(geojsonSchema).json(file_path, multiLine=True)\n",
    "    \n",
    "    df = (df\n",
    "        .select(F.explode(\"features\").alias(\"features\"))\n",
    "        .select(\"features.*\")\n",
    "        .withColumn(\"geometry\", F.expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "        )\n",
    "    \n",
    "    counties_dataframes[file_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeneralManufacturingFacilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'GeneralManufacturingFacilities'\n",
    "\n",
    "dataset_nodes_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/dataset_graphs/{dataset}Nodes.csv\"\n",
    "dataset_edges_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/dataset_graphs/{dataset}Edges.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = get_file_to_df(f'{dataset}.geojson')\n",
    "df_dataset.show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"properties\": {\n",
    "#     \"OBJECTID\": 3,\n",
    "#     \"UNIQUE_ID\": \"N/A\",\n",
    "#     \"NAME\": \"JWS REFRIGERATION & AIR CONDITIONING\",\n",
    "#     \"PHONE\": \"(671) 646-7662\",\n",
    "#     \"FAX\": \"NOT AVAILABLE\",\n",
    "#     \"ADDRESS\": \"290 TUN JOSE SALAS STREET\",\n",
    "#     \"ADDRESS2\": \"SUITE A\",\n",
    "#     \"CITY\": \"TAMUNING\",\n",
    "#     \"STATE\": \"GU\",\n",
    "#     \"ZIP\": \"96913\",\n",
    "#     \"ZIP4\": \"N/A\",\n",
    "#     \"COUNTY\": \"GUAM\",\n",
    "#     \"FIPS\": \"66010\",\n",
    "#     \"MADDRESS\": \"290 TUN JOSE SALAS STREET\",\n",
    "#     \"MCITY\": \"TAMUNING\",\n",
    "#     \"MSTATE\": \"GU\",\n",
    "#     \"MZIP\": \"96913\",\n",
    "#     \"MZIP4\": \"N/A\",\n",
    "#     \"DIRECTIONS\": \"NOT AVAILABLE\",\n",
    "#     \"GEOPREC\": \"BLOCKFACE\",\n",
    "#     \"EMP\": 0,\n",
    "#     \"PRODUCT\": \"REFRIGERATION AND AIR-CONDITIONING\",\n",
    "#     \"SIC\": \"3585\",\n",
    "#     \"SIC2\": \"2542\",\n",
    "#     \"SIC3\": \"N/A\",\n",
    "#     \"SIC4\": \"N/A\",\n",
    "#     \"NAICS\": \"N/A\",\n",
    "#     \"NAICSDESCR\": \"NOT AVAILABLE\",\n",
    "#     \"WEB\": \"WWW.JWSGUAM.COM\",\n",
    "#     \"LONGITUDE\": 144.7883065,\n",
    "#     \"LATITUDE\": 13.4912295\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df_dataset.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"GeneralManufacturingFacility\")) \\\n",
    "                 .withColumn(\"Name\", col(\"properties.NAME\")) \\\n",
    "                 .withColumn(\"Phone\", col(\"properties.PHONE\")) \\\n",
    "                 .withColumn(\"Fax\", col(\"properties.FAX\")) \\\n",
    "                 .withColumn(\"Address\", col(\"properties.ADDRESS\")) \\\n",
    "                 .withColumn(\"City\", col(\"properties.CITY\")) \\\n",
    "                 .withColumn(\"State\", col(\"properties.STATE\")) \\\n",
    "                 .withColumn(\"Zip\", col(\"properties.ZIP\")) \\\n",
    "                 .withColumn(\"County\", col(\"properties.COUNTY\"))\n",
    "\n",
    "df_nodes = df_nodes.withColumn(\"geometry\", expr(\"ST_AsGeoJSON(geometry)\"))\n",
    "\n",
    "df_nodes = df_nodes.drop(\"properties\")\n",
    "\n",
    "df_nodes.show(n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv(df_nodes, dataset_nodes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_prefix = \"BlockGroup_\"\n",
    "\n",
    "final_block_edges = {}\n",
    "final_block_nodes = {}\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    blocks = blocks_file_name.replace('.geojson', '')\n",
    "    print(blocks)\n",
    "    df_blocks = blocks_dataframes[blocks_file_name]\n",
    "\n",
    "    df_blocks_contain_dataset = create_relationships_point(df_blocks, df_dataset)\n",
    "    \n",
    "    df_dataset_isPartOf_block_edges = create_edge_df(df_blocks_contain_dataset, concat(lit(dataset_prefix), col(\"child_properties.NAME\")), lit(\"isPartOf\"), concat(lit(block_prefix), col(\"parent_properties.GEOID\")))\n",
    "    # df_dataset_isPartOf_block_edges.show(n=1, truncate=False)\n",
    "    final_block_edges[f'DatasetIsPartOf{blocks}Blocks'] = df_dataset_isPartOf_block_edges\n",
    "\n",
    "    df_block_contains_dataset_edges = create_edge_df(df_blocks_contain_dataset, concat(lit(block_prefix), col(\"parent_properties.GEOID\")), lit(\"Contains\"), concat(lit(dataset_prefix), col(\"child_properties.NAME\")))\n",
    "    # df_block_contains_dataset_edges.show(n=1, truncate=False)\n",
    "    final_block_edges[f'{blocks}BlockContainsDataset'] = df_block_contains_dataset_edges\n",
    "\n",
    "    df_nodes = create_node_df(df_dataset_isPartOf_block_edges, lit(dataset))\n",
    "    df_nodes = df_nodes.filter(~col(\"Node_ID\").contains(f\"{block_prefix}_\"))\n",
    "    final_block_nodes[f'{blocks}Blocks'] = df_nodes\n",
    "    # df_nodes.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_block_edges_df = union_all(final_block_edges)\n",
    "combined_block_nodes_df = union_all(final_block_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_csv(combined_block_nodes_df, base_nodes_path) # Append to Base Graph\n",
    "append_to_csv(combined_block_edges_df, base_edges_path) # Append to Base Graph\n",
    "create_csv(combined_block_edges_df, dataset_nodes_path) # Create Dataset Edges File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HydroCarbonGasLiquidPipelines (Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'HydroCarbonGasLiquidPipelines'\n",
    "\n",
    "dataset_nodes_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/dataset_graphs/{dataset}Nodes.csv\"\n",
    "dataset_edges_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/dataset_graphs/{dataset}Edges.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = get_file_to_df(f'{dataset}.geojson')\n",
    "df_dataset.show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"properties\": {\n",
    "#     \"FID\": 28,\n",
    "#     \"Opername\": \"ENERGY TRANSFER\",\n",
    "#     \"Pipename\": \"West Texas Pipeline\",\n",
    "#     \"Shape_Leng\": 9.96270805248,\n",
    "#     \"Shape__Length\": 1148322.03802495\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df_dataset.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.Pipename\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"HydroCarbonGasLiquidPipelines\")) \\\n",
    "                 .withColumn(\"Opername\", col(\"properties.Opername\")) \\\n",
    "                 .withColumn(\"FID\", col(\"properties.FID\")) \n",
    "\n",
    "df_nodes = df_nodes.withColumn(\"geometry\", expr(\"ST_AsGeoJSON(geometry)\"))\n",
    "\n",
    "df_nodes = df_nodes.drop(\"properties\")\n",
    "\n",
    "df_nodes.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv(df_nodes, dataset_nodes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_prefix = \"BlockGroup_\"\n",
    "\n",
    "final_block_edges = {}\n",
    "final_block_nodes = {}\n",
    "\n",
    "for blocks_file_name, df_blocks in blocks_dataframes.items():\n",
    "    blocks = blocks_file_name.replace('.geojson', '')\n",
    "    print(blocks)\n",
    "    df_blocks_contain_dataset = create_relationships_geometry(df_blocks, df_dataset)\n",
    "    \n",
    "    df_dataset_isPartOf_block_edges = create_edge_df(df_blocks_contain_dataset, concat(lit(dataset_prefix), col(\"child_properties.Pipename\")), lit(\"isPartOf\"), concat(lit(block_prefix), col(\"parent_properties.GEOID\")))\n",
    "    # df_dataset_isPartOf_block_edges.show(n=1, truncate=False)\n",
    "    final_block_edges[f'DatasetIsPartOf{blocks}Blocks'] = df_dataset_isPartOf_block_edges\n",
    "\n",
    "    df_block_contains_dataset_edges = create_edge_df(df_blocks_contain_dataset, concat(lit(block_prefix), col(\"parent_properties.GEOID\")), lit(\"Contains\"), concat(lit(dataset_prefix), col(\"child_properties.Pipename\")))\n",
    "    # df_block_contains_dataset_edges.show(n=1, truncate=False)\n",
    "    final_block_edges[f'{blocks}BlockContainsDataset'] = df_block_contains_dataset_edges\n",
    "\n",
    "    df_nodes = create_node_df(df_block_contains_dataset_edges, lit(dataset))\n",
    "    df_nodes = df_nodes.filter(~col(\"Node_ID\").contains(f\"{dataset_prefix}_\"))\n",
    "    final_block_nodes[f'{blocks}Blocks'] = df_nodes\n",
    "    # df_nodes.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_block_edges_df = union_all(final_block_edges)\n",
    "combined_block_nodes_df = union_all(final_block_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_block_edges_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_block_nodes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_csv(combined_block_nodes_df, base_nodes_path) # Append to Base Graph\n",
    "append_to_csv(combined_block_edges_df, base_edges_path) # Append to Base Graph\n",
    "create_csv(combined_block_edges_df, dataset_nodes_path) # Create Dataset Edges File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HydroCarbonGasLiquidPipelines (Counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_prefix = \"County_\"\n",
    "\n",
    "final_county_edges = {}\n",
    "final_county_nodes = {}\n",
    "\n",
    "for counties_file_name, df_counties in counties_dataframes.items():\n",
    "    counties = counties_file_name.replace('.geojson', '')\n",
    "    print(counties)\n",
    "\n",
    "    df_counties_contain_dataset = create_relationships_geometry(df_counties, df_dataset)\n",
    "    \n",
    "    df_dataset_isPartOf_counties_edges = create_edge_df(df_counties_contain_dataset, concat(lit(dataset_prefix), col(\"child_properties.Pipename\")), lit(\"isPartOf\"), concat(lit(county_prefix), col(\"parent_properties.NAME\")))\n",
    "    df_dataset_isPartOf_counties_edges.show(n=1, truncate=False)\n",
    "    final_county_edges[f'DatasetIsPartOf{counties}Counties'] = df_dataset_isPartOf_counties_edges\n",
    "\n",
    "    df_county_contains_dataset_edges = create_edge_df(df_counties_contain_dataset, concat(lit(county_prefix), col(\"parent_properties.NAME\")), lit(\"Contains\"), concat(lit(dataset_prefix), col(\"child_properties.Pipename\")))\n",
    "    df_county_contains_dataset_edges.show(n=1, truncate=False)\n",
    "    final_county_edges[f'{counties}CountiesContainsDataset'] = df_county_contains_dataset_edges\n",
    "\n",
    "    df_nodes = create_node_df(df_county_contains_dataset_edges, lit(dataset))\n",
    "    df_nodes = df_nodes.filter(~col(\"Node_ID\").contains(f\"{dataset_prefix}_\"))\n",
    "    final_county_nodes[f'{blocks}Blocks'] = df_nodes\n",
    "    df_nodes.show(truncate=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_county_edges_df = union_all(final_county_edges)\n",
    "combined_county_nodes_df = union_all(final_county_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_county_edges_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_county_nodes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_to_csv(combined_county_nodes_df, base_nodes_path) # Append to Base Graph\n",
    "append_to_csv(combined_county_edges_df, base_edges_path) # Append to Base Graph\n",
    "create_csv(combined_county_edges_df, dataset_nodes_path) # Create Dataset Edges File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Refineries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'OilRefineries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'OilRefineries'\n",
    "\n",
    "# \"properties\": {\n",
    "#     \"OBJECTID\": 18,\n",
    "#     \"REF_ID\": \"REF220020\",\n",
    "#     \"NAME\": \"LAKE CHARLES\",\n",
    "#     \"ADDRESS\": \"1601 HWY 108 E\",\n",
    "#     \"CITY\": \"SULPHUR\",\n",
    "#     \"STATE\": \"LA\",\n",
    "#     \"ZIP\": \"70665\",\n",
    "#     \"ZIP4\": \"NOT AVAILABLE\",\n",
    "#     \"TELEPHONE\": \"(337) 708-8431\",\n",
    "#     \"TYPE\": \"MODERN DEEP-CONVERSION FACILITY\",\n",
    "#     \"STATUS\": \"IN SERVICE\",\n",
    "#     \"POPULATION\": 1183,\n",
    "#     \"COUNTY\": \"CALCASIEU\",\n",
    "#     \"COUNTYFIPS\": \"22019\",\n",
    "#     \"COUNTRY\": \"USA\",\n",
    "#     \"LATITUDE\": 30.17866697000005,\n",
    "#     \"LONGITUDE\": -93.33023517799995,\n",
    "#     \"NAICS_CODE\": \"324110\",\n",
    "#     \"NAICS_DESC\": \"PETROLEUM REFINERIES\",\n",
    "#     \"SOURCE\": \"EIA-820; EPA TRI\",\n",
    "#     \"SOURCEDATE\": \"2017/01/01 00:00:00\",\n",
    "#     \"VAL_METHOD\": \"IMAGERY/OTHER\",\n",
    "#     \"VAL_DATE\": \"2018/01/31 00:00:00\",\n",
    "#     \"WEBSITE\": \"http://citgorefining.com\",\n",
    "#     \"OWNER\": \"PDV AMERICA INC\",\n",
    "#     \"OPERNAME\": \"CITGO PETROLEUM CORP\",\n",
    "#     \"RMP_ID\": \"55717\",\n",
    "#     \"EPA_ID\": \"100000140199\",\n",
    "#     \"POSREL\": \"WITHIN 166 FEET\",\n",
    "#     \"CAPACITY\": 425000,\n",
    "#     \"US_RANK\": 6,\n",
    "#     \"CRUDE\": 425000,\n",
    "#     \"VACDIST\": 230000,\n",
    "#     \"COKING\": 85410,\n",
    "#     \"THERMALOP\": 0,\n",
    "#     \"CATCRACK\": 143000,\n",
    "#     \"CATREFORM\": 103035,\n",
    "#     \"CATHYDCRCK\": 46000,\n",
    "#     \"CATHYDTRT\": 398200,\n",
    "#     \"ALKY\": 26400,\n",
    "#     \"POLDIM\": 0,\n",
    "#     \"AROMATIC\": 20900,\n",
    "#     \"ISOMER\": 28000,\n",
    "#     \"LUBES\": 0,\n",
    "#     \"OXYGENATES\": 0,\n",
    "#     \"HYDRGN\": 0,\n",
    "#     \"COKE\": 32820,\n",
    "#     \"SULFUR\": 717,\n",
    "#     \"ASPHALT\": 0\n",
    "# }\n",
    "\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"OilRefineries\")) \\\n",
    "                 .withColumn(\"CAPACITY\", col(\"properties.CAPACITY\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaturalGasStorageFacilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'NaturalGasStorageFacilities'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"properties\": {\n",
    "#     \"FID\": 12,\n",
    "#     \"STFID\": \"STF190012\",\n",
    "#     \"NAME\": \"COLUMBUS CITY\",\n",
    "#     \"ADDRESS\": \"120TH STREET/S AVE\",\n",
    "#     \"CITY\": \"COLUMBUS CITY\",\n",
    "#     \"STATE\": \"IA\",\n",
    "#     \"ZIP\": \"52738\",\n",
    "#     \"ZIP4\": \"NOT AVAILABLE\",\n",
    "#     \"TELEPHONE\": \"NOT AVAILABLE\",\n",
    "#     \"TYPE\": \"AQUIFER\",\n",
    "#     \"STATUS\": \"ACTIVE\",\n",
    "#     \"POPULATION\": -999,\n",
    "#     \"COUNTY\": \"LOUISA\",\n",
    "#     \"COUNTYFIPS\": \"19115\",\n",
    "#     \"COUNTRY\": \"USA\",\n",
    "#     \"LATITUDE\": 41.234864,\n",
    "#     \"LONGITUDE\": -91.350155,\n",
    "#     \"NAICS_CODE\": \"486210\",\n",
    "#     \"NAICS_DESC\": \"STORAGE OF NATURAL GAS\",\n",
    "#     \"SOURCE\": \"EIA, IMAGERY\",\n",
    "#     \"SOURCEDATE\": \"2018/12/01 00:00:00\",\n",
    "#     \"VAL_METHOD\": \"IMAGERY/OTHER\",\n",
    "#     \"VAL_DATE\": \"2019/04/10 00:00:00\",\n",
    "#     \"WEBSITE\": \"http://www.kindermorgan.com/\",\n",
    "#     \"EPAID\": \"155321\",\n",
    "#     \"OWNER\": \"KINDER MORGAN (NATURAL GAS PIPELINE CO OF AMERICA)\",\n",
    "#     \"OPERATOR\": \"NATURAL GAS PIPELINE CO OF AMERICA\",\n",
    "#     \"POSREL\": \"EXCEEDS 1 MILE\",\n",
    "#     \"OWNERPCT\": 100,\n",
    "#     \"MAXDEL\": 175000,\n",
    "#     \"WORKCAP\": 16685000,\n",
    "#     \"BASEGAS\": 37700000,\n",
    "#     \"TOTALCAP\": 54400193,\n",
    "#     \"REGION\": \"MIDWEST REGION\",\n",
    "#     \"PROPMAX\": -999,\n",
    "#     \"PROPWORK\": -999,\n",
    "#     \"PROPTOTAL\": -999,\n",
    "#     \"RESERVNAME\": \"GALESVILLE MT. SIMON ST. PETER\",\n",
    "#     \"SEC_NAICS\": \"NOT APPLICABLE\",\n",
    "#     \"SEC_N_DESC\": \"NOT APPLICABLE\"\n",
    "# }\n",
    "\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"NaturalGasStorageFacilities\")) \\\n",
    "                 .withColumn(\"Total Capacity\", col(\"properties.TOTALCAP\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaturalGasProcessingPlants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'NaturalGasProcessingPlants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"properties\": {\n",
    "#     \"OBJECTID\": 5,\n",
    "#     \"NGPPID\": \"NGPP010177\",\n",
    "#     \"NAME\": \"DOGWOOD OAKS PLANT\",\n",
    "#     \"ADDRESS\": \"21680 HWY 41\",\n",
    "#     \"CITY\": \"BREWTON\",\n",
    "#     \"STATE\": \"AL\",\n",
    "#     \"ZIP\": \"36426\",\n",
    "#     \"ZIP4\": \"NOT AVAILABLE\",\n",
    "#     \"TELEPHONE\": \"(251) 248-2903\",\n",
    "#     \"TYPE\": \"NATURAL GAS PROCESSING PLANT\",\n",
    "#     \"STATUS\": \"ACTIVE\",\n",
    "#     \"POPULATION\": 12,\n",
    "#     \"COUNTY\": \"ESCAMBIA\",\n",
    "#     \"COUNTYFIPS\": \"01053\",\n",
    "#     \"COUNTRY\": \"USA\",\n",
    "#     \"LATITUDE\": 31.243471,\n",
    "#     \"LONGITUDE\": -87.187836,\n",
    "#     \"NAICS_CODE\": \"211130\",\n",
    "#     \"NAICS_DESC\": \"NATURAL GAS EXTRACTION\",\n",
    "#     \"SOURCE\": \"EPA RISK MANAGEMENT PLAN (RMP) - THE RIGHT-TO-KNOW NETWORK\",\n",
    "#     \"SOURCEDATE\": \"2013/03/22 00:00:00\",\n",
    "#     \"VAL_METHOD\": \"IMAGERY/OTHER\",\n",
    "#     \"VAL_DATE\": \"2015/06/17 00:00:00\",\n",
    "#     \"WEBSITE\": \"www.plainsallamerican.com/\",\n",
    "#     \"FACID\": \"100000218356\",\n",
    "#     \"COMPNAME\": \"PLAINS GAS SOLUTIONS, LLC\",\n",
    "#     \"POSREL\": \"WITHIN 40 FEET\",\n",
    "#     \"OPERATOR\": \"CDM MAX, L.L.C. (PLAINS GAS SOLUTIONS, LLC)\",\n",
    "#     \"OPERADDR\": \"333 CLAY STREET, SUITE 1600\",\n",
    "#     \"OPERCITY\": \"HOUSTON\",\n",
    "#     \"OPERSTATE\": \"TX\",\n",
    "#     \"OPERCNTRY\": \"USA\",\n",
    "#     \"OPERZIP\": \"77002\",\n",
    "#     \"OPERPHONE\": \"(251) 248-2903\",\n",
    "#     \"OPERURL\": \"www.plainsallamerican.com/about-us/subsidiary-websites/plains-gas-solutions/facilities\",\n",
    "#     \"GASCAP\": 4,\n",
    "#     \"PROCAMTBLS\": 186840,\n",
    "#     \"BASIN\": \"GULF COAST COAL REGION\",\n",
    "#     \"PLANTFLOW\": 4,\n",
    "#     \"BTUCONTENT\": 1000,\n",
    "#     \"GASSTORCAP\": -999,\n",
    "#     \"LIQSTORCAP\": 1000,\n",
    "#     \"RMP_ID\": \"1000032802\",\n",
    "#     \"EPA_ID\": \"110055375883\"\n",
    "# }\n",
    "\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"NaturalGasProcessingPlants\")) \\\n",
    "                 .withColumn(\"Plant Flow\", col(\"properties.PLANTFLOW\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaturalGasCompressorStations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'GeographicRegions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"properties\": { \n",
    "#     \"scalerank\": 7.0, \n",
    "#     \"featurecla\": \"Island\", \n",
    "#     \"name\": \"Adak\", \n",
    "#     \"namealt\": null, \n",
    "#     \"region\": \"North America\", \n",
    "#     \"subregion\": null \n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.name\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"GeographicRegions\")) \\\n",
    "                 .withColumn(\"Scale Rank\", col(\"properties.scalerank\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PowerPlants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'PowerPlants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"properties\": {\n",
    "#     \"PGM_SYS_AC\": \"EIA-860\",\n",
    "#     \"PGM_SYS_ID\": \"124\",\n",
    "#     \"REGISTRY_I\": \"110002569569\",\n",
    "#     \"PRIMARY_NA\": \"TUCSON ELECTRIC POWER DEMOSS-PETRIE DSL\",\n",
    "#     \"LOCATION_A\": \"2501 NORTH FLOWING WELLS ROAD\",\n",
    "#     \"CITY_NAME\": \"TUCSON\",\n",
    "#     \"COUNTY_NAM\": \"PIMA\",\n",
    "#     \"STATE_CODE\": \"AZ\",\n",
    "#     \"POSTAL_COD\": \"85705-4015\",\n",
    "#     \"FEDERAL_FA\": \"N\",\n",
    "#     \"TRIBAL_LAN\": \"\",\n",
    "#     \"DATA_QUALI\": \"V\",\n",
    "#     \"LAST_REPOR\": \"\",\n",
    "#     \"CREATE_DAT\": \"2000-03-01\",\n",
    "#     \"UPDATE_DAT\": \"2014-04-30\",\n",
    "#     \"LATITUDE83\": 32.2523193359375,\n",
    "#     \"LONGITUDE8\": -110.99149322509766,\n",
    "#     \"REF_POINT_\": \"CENTER OF A FACILITY OR STATION\",\n",
    "#     \"DERIVED_HU\": \"15050301\",\n",
    "#     \"DERIVED_WB\": \"150503010906\",\n",
    "#     \"DERIVED_CB\": \"040190045044008\",\n",
    "#     \"DERIVED_CD\": \"02\",\n",
    "#     \"OZONE_8HR_\": \"\",\n",
    "#     \"PB_2008_AR\": \"\",\n",
    "#     \"PM25_1997_\": \"\",\n",
    "#     \"PM25_2006_\": \"\",\n",
    "#     \"OZONE_8H_1\": \"\",\n",
    "#     \"UTILITY_ID\": \"24211\",\n",
    "#     \"UTILITY_NA\": \"Tucson Electric Power Co\",\n",
    "#     \"PLANT_CODE\": \"124\",\n",
    "#     \"PLANT_NAME\": \"Demoss Petrie\",\n",
    "#     \"GENERATOR_\": \"GT2\",\n",
    "#     \"PRIME_MOVE\": \"GT\",\n",
    "#     \"STATUS\": \"OP\",\n",
    "#     \"NAMEPLATE\": 85,\n",
    "#     \"SUMMER_CAP\": 72.19999694824219,\n",
    "#     \"WINTER_CAP\": 83.30000305175781,\n",
    "#     \"UNIT_CODE\": \"\",\n",
    "#     \"OPERATING_\": \"6\",\n",
    "#     \"OPERATIN_1\": \"2001\",\n",
    "#     \"ENERGY_SOU\": \"NG\",\n",
    "#     \"ENERGY_S_1\": \"\",\n",
    "#     \"ENERGY_S_2\": \"\",\n",
    "#     \"ENERGY_S_3\": \"\",\n",
    "#     \"ENERGY_S_4\": \"\",\n",
    "#     \"ENERGY_S_5\": \"\",\n",
    "#     \"MULTIPLE_F\": \"N\",\n",
    "#     \"DELIVER_PO\": \"Y\",\n",
    "#     \"SYNCHRONIZ\": \"\",\n",
    "#     \"OWNERSHIP\": \"S\",\n",
    "#     \"TURBINES\": \".\",\n",
    "#     \"COGENERATO\": \"N\",\n",
    "#     \"SECTOR_NAM\": \"Electric Utility\",\n",
    "#     \"SECTOR\": \"1\",\n",
    "#     \"TOPPING_BO\": \"\",\n",
    "#     \"DUCT_BURNE\": \"N\",\n",
    "#     \"PLANNED_MO\": \"N\",\n",
    "#     \"PLANNED_UP\": \".\",\n",
    "#     \"PLANNED__1\": \".\",\n",
    "#     \"PLANNED__2\": \".\",\n",
    "#     \"PLANNED__3\": \".\",\n",
    "#     \"PLANNED_DE\": \".\",\n",
    "#     \"PLANNED__4\": \".\",\n",
    "#     \"PLANNED__5\": \".\",\n",
    "#     \"PLANNED__6\": \".\",\n",
    "#     \"PLANNED_NE\": \"\",\n",
    "#     \"PLANNED_EN\": \"\",\n",
    "#     \"PLANNED_RE\": \".\",\n",
    "#     \"PLANNED__7\": \".\",\n",
    "#     \"OTHER_MODS\": \"\",\n",
    "#     \"OTHER_MOD_\": \".\",\n",
    "#     \"OTHER_MO_1\": \".\",\n",
    "#     \"PLANNED__8\": \".\",\n",
    "#     \"PLANNED__9\": \".\",\n",
    "#     \"SFG_SYSTEM\": \"N\",\n",
    "#     \"PULVERIZED\": \"\",\n",
    "#     \"FLUIDIZED_\": \"\",\n",
    "#     \"SUBCRITICA\": \"\",\n",
    "#     \"SUPERCRITI\": \"\",\n",
    "#     \"ULTRASUPER\": \"\",\n",
    "#     \"CARBONCAPT\": \"\",\n",
    "#     \"STARTUP_SO\": \"\",\n",
    "#     \"STARTUP__1\": \"\",\n",
    "#     \"STARTUP__2\": \"\",\n",
    "#     \"STARTUP__3\": \"\",\n",
    "#     \"ENERGY_SRC\": \"Natural Gas\",\n",
    "#     \"ENERGY_S_6\": \"\"\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.PRIMARY_NA\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"PowerPlants\")) \\\n",
    "                 .withColumn(\"Summer Capacity\", col(\"properties.SUMMER_CAP\")) \\\n",
    "                .withColumn(\"Winter Capacity\", col(\"properties.WINTER_CAP\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaturalGasPipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'NaturalGasPipelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"properties\": {\n",
    "#     \"TYPEPIPE\": \"Intrastate\",\n",
    "#     \"Operator\": \"Crosstex Texas Systems\",\n",
    "#     \"Shape_Leng\": 0.00187974387,\n",
    "#     \"Shape__Len\": 240.3441469695\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.Operator\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"NaturalGasPipelines\")) \\\n",
    "                 .withColumn(\"Pipe Type\", col(\"properties.TYPEPIPE\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Dams'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"properties\": {\n",
    "#     \"OBJECTID\": 9144,\n",
    "#     \"NGAID\": \"10130292\",\n",
    "#     \"METLNKID\": \"\",\n",
    "#     \"FEATTYPE\": \"POLYLINE\",\n",
    "#     \"NAME\": \"EL PASO DAM NO 10\",\n",
    "#     \"CITY\": \"EL PASO\",\n",
    "#     \"STATE\": \"TX\",\n",
    "#     \"COUNTY\": \"EL PASO\",\n",
    "#     \"FIPS\": \"48141\",\n",
    "#     \"DIRECTIONS\": \"\",\n",
    "#     \"EMERGTITLE\": \"\",\n",
    "#     \"EMERGPHONE\": \"\",\n",
    "#     \"EMERGEXT\": \"\",\n",
    "#     \"CONTDATE\": \"1899-11-30T00:00:00.000Z\",\n",
    "#     \"CONTHOW\": \"\",\n",
    "#     \"GEODATE\": \"2007-03-28T00:00:00.000Z\",\n",
    "#     \"GEOHOW\": \"MANUAL\",\n",
    "#     \"HSIPTHEMES\": \"CRITICAL INFRASTUCTURE, PDD-63; WATER SUPPLY; DAMS\",\n",
    "#     \"SOURCE\": \"USACE\",\n",
    "#     \"X\": -106.4814352,\n",
    "#     \"Y\": 31.7778363,\n",
    "#     \"QC_QA\": \"\",\n",
    "#     \"RECORDID\": \"72827\",\n",
    "#     \"OTHER_NAME\": \"\",\n",
    "#     \"FORM_NAME\": \"\",\n",
    "#     \"STATEID\": \"\",\n",
    "#     \"NIDID\": \"TX07023\",\n",
    "#     \"SECTION\": \"3106-432\",\n",
    "#     \"RIVER\": \"OFF CH-RIO GRANDE\",\n",
    "#     \"CITYAFFECT\": \"EL PASO\",\n",
    "#     \"NIDSTATE\": \"TX\",\n",
    "#     \"NIDCOUNTY\": \"EL PASO\",\n",
    "#     \"DISTANCE\": 0,\n",
    "#     \"OWN_TYPE\": \"L\",\n",
    "#     \"PRIV_DAM\": \"\",\n",
    "#     \"DAM_TYPE\": \"RE\",\n",
    "#     \"CORE\": \"XX\",\n",
    "#     \"FOUND\": \"U\",\n",
    "#     \"PURPOSES\": \"C\",\n",
    "#     \"YR_COMPL\": \"\",\n",
    "#     \"YR_MOD\": \"\",\n",
    "#     \"DAM_LENGTH\": 0,\n",
    "#     \"DAM_HEIGHT\": 30,\n",
    "#     \"STR_HEIGHT\": 0,\n",
    "#     \"HYD_HEIGHT\": 0,\n",
    "#     \"NID_HEIGHT\": 30,\n",
    "#     \"MAX_DIS\": 0,\n",
    "#     \"MAX_STOR\": 24,\n",
    "#     \"NORMAL_STO\": 0,\n",
    "#     \"NID_STOR\": 24,\n",
    "#     \"SURF_AREA\": 0,\n",
    "#     \"DRAIN_AREA\": 0,\n",
    "#     \"HAZARD\": \"H\",\n",
    "#     \"EAP\": \"N\",\n",
    "#     \"INSP_DATE\": \"1996-07-17T00:00:00.000Z\",\n",
    "#     \"INSP_FREQU\": 0,\n",
    "#     \"ST_REG_DAM\": \"Y\",\n",
    "#     \"ST_REG_AG\": \"\",\n",
    "#     \"SPILL_TYPE\": \"U\",\n",
    "#     \"SPILL_WIDT\": 12,\n",
    "#     \"OUT_GATES\": \"\",\n",
    "#     \"VOLUME\": 0,\n",
    "#     \"NO_LOCKS\": 0,\n",
    "#     \"LEN_LOCKS\": 0,\n",
    "#     \"WID_LOCKS\": 0,\n",
    "#     \"FED_FUND\": \"\",\n",
    "#     \"FED_DESIGN\": \"\",\n",
    "#     \"FED_CONSTR\": \"\",\n",
    "#     \"FED_REG\": \"\",\n",
    "#     \"FED_INSP\": \"\",\n",
    "#     \"FED_OPER\": \"\",\n",
    "#     \"FED_OWN\": \"\",\n",
    "#     \"FED_OTHER\": \"\",\n",
    "#     \"SOURCE_A\": \"TX\",\n",
    "#     \"SUB_DATE\": \"20000401\",\n",
    "#     \"URL_ADDRES\": \"HTTP://WWW.TCEQ.STATE.TX.US/\",\n",
    "#     \"CONG_DIST\": \"TX16\",\n",
    "#     \"SHAPE_Leng\": 216.77816378547902\n",
    "# }\n",
    "\n",
    "dataset_prefix = lit(dataset + \"_\")\n",
    "\n",
    "# Attach all properties (Node_ID is the forein id)\n",
    "df_nodes = df.withColumn(\"Node_ID\", concat(dataset_prefix, col(\"properties.NAME\"))) \\\n",
    "                 .withColumn(\"Type\", lit(\"Dams\")) \\\n",
    "                 .withColumn(\"River\", col(\"properties.RIVER\")) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
