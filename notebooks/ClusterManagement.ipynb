{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cluster Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooke is to assist with managing the your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Machines\n",
    "\n",
    "This is from the canvas announcement.\n",
    "\n",
    "### Freddy\n",
    "\n",
    "**Ports:** [30785, 30799] \n",
    "\n",
    "1. columbia \n",
    "2. columbus-oh \n",
    "3. concord \n",
    "4. denver \n",
    "5. des-moines \n",
    "6. dover \n",
    "7. frankfort \n",
    "8. harrisburg \n",
    "9. hartford \n",
    "10. helena \n",
    "11. honolulu \n",
    "12. indianapolis \n",
    "\n",
    "### Eric\n",
    "\n",
    "*idk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your user name\n",
    "username = 'flarrieu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columbus-oh\n",
      "concord\n",
      "columbia\n",
      "denver\n",
      "des-moines\n",
      "dover\n",
      "frankfort\n",
      "harrisburg\n",
      "30785\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Current Machines and port in use\n",
    "\n",
    "with open(f'/s/chopin/a/grad/{username}/bigdata_configs/machines_and_ports.txt') as f: \n",
    "    s = f.read()\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- namenode: columbus-oh\n",
      "\n",
      "- secondary namenode: concord\n",
      "\n",
      "- resource manager: columbia\n",
      "\n",
      "- datanode1: denver\n",
      "\n",
      "- datanode2: des-moines\n",
      "\n",
      "- datanode3: dover\n",
      "\n",
      "- datanode4: frankfort\n",
      "\n",
      "- datanode5: harrisburg\n"
     ]
    }
   ],
   "source": [
    "# Machine Roles After Cluster Setup\n",
    "\n",
    "with open(f'/s/chopin/a/grad/{username}/bigdata_configs/README.md', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "machine_roles = lines[-8:]\n",
    "\n",
    "for machine in machine_roles:\n",
    "    print(machine)\n",
    "    if \"namenode\" in machine and not \"secondary\" in machine:\n",
    "        namenode = machine.split(\" \")[2]\n",
    "    if \"resource manager\" in machine:\n",
    "        resource_manager = machine.split(\" \")[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Current Cluster Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/Modules/init/bash: line 104: unalias: pip3: not found\n",
      "/usr/share/Modules/init/bash: line 105: unalias: pip: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columbus-oh\n",
      "45044 Jps\n",
      "21195 NameNode\n",
      "23244 Master\n",
      "concord\n",
      "748746 Jps\n",
      "633010 SecondaryNameNode\n",
      "columbia\n",
      "1032107 Jps\n",
      "703039 ResourceManager\n",
      "denver\n",
      "1902285 Jps\n",
      "1399813 NodeManager\n",
      "1406144 Worker\n",
      "1405900 Worker\n",
      "1397038 DataNode\n",
      "des-moines\n",
      "441463 NodeManager\n",
      "442949 Worker\n",
      "440611 DataNode\n",
      "443072 Worker\n",
      "466794 Jps\n",
      "dover\n",
      "2339026 DataNode\n",
      "2344384 Worker\n",
      "2344532 Worker\n",
      "2340746 NodeManager\n",
      "2473386 Jps\n",
      "frankfort\n",
      "417317 DataNode\n",
      "419651 Worker\n",
      "419776 Worker\n",
      "441407 Jps\n",
      "418169 NodeManager\n",
      "harrisburg\n",
      "1850804 Jps\n",
      "1626417 Worker\n",
      "1620452 NodeManager\n",
      "1626582 Worker\n",
      "1618726 DataNode\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Print the current status of the cluster\n",
    "# You will need to set the following in your ~/.bashrc: \n",
    "#   chmod +x /s/chopin/a/grad/{username}/hadoopConf/monitor.sh\n",
    "#   export MONITOR_CLUSTER=\"/s/chopin/a/grad/{username}/hadoopConf/monitor.sh\"\n",
    "\n",
    "source ~/.bashrc\n",
    "$MONITOR_CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Starting DFS and Yarn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: paramiko in /s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages (from paramiko) (4.1.2)\n",
      "Requirement already satisfied: cryptography>=3.3 in /s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages (from paramiko) (42.0.5)\n",
      "Requirement already satisfied: pynacl>=1.5 in /s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages (from paramiko) (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/python-env/py39/lib/python3.9/site-packages (from cryptography>=3.3->paramiko) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/python-env/py39/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko) (2.21)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib64/python3.9/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/lib64/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib64/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/__main__.py\", line 31, in <module>\n",
      "    sys.exit(_main())\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 952, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2', new='24.0'),)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import paramiko\n",
    "\n",
    "# You will need to set the following in your ~/.bashrc: \n",
    "#   export PASSWORD=\"your password\"\n",
    "\n",
    "password = os.getenv(\"PASSWORD\")\n",
    "\n",
    "if not password:\n",
    "    print(\"Error: You need to set your password in you bashrc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh flarrieu@columbus-oh\n",
      ".cs.colostate.edu\n",
      "Starting namenodes on [columbus-oh.cs.colostate.edu]\n",
      "columbus-oh.cs.colostate.edu: namenode is running as process 21195.  Stop it first and ensure /tmp/hadoop-flarrieu-namenode.pid file is empty before retry.\n",
      "Starting datanodes\n",
      "harrisburg: datanode is running as process 1618726.  Stop it first and ensure /tmp/hadoop-flarrieu-datanode.pid file is empty before retry.\n",
      "dover: datanode is running as process 2339026.  Stop it first and ensure /tmp/hadoop-flarrieu-datanode.pid file is empty before retry.\n",
      "denver: datanode is running as process 1397038.  Stop it first and ensure /tmp/hadoop-flarrieu-datanode.pid file is empty before retry.\n",
      "frankfort: datanode is running as process 417317.  Stop it first and ensure /tmp/hadoop-flarrieu-datanode.pid file is empty before retry.\n",
      "des-moines: datanode is running as process 440611.  Stop it first and ensure /tmp/hadoop-flarrieu-datanode.pid file is empty before retry.\n",
      "Starting secondary namenodes [concord.cs.colostate.edu]\n",
      "concord.cs.colostate.edu: secondarynamenode is running as process 633010.  Stop it first and ensure /tmp/hadoop-flarrieu-secondarynamenode.pid file is empty before retry.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start DFS\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/start-dfs.sh') # Run start-dfs.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting resourcemanager\n",
      "Starting nodemanagers\n",
      "frankfort: nodemanager is running as process 418169.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "dover: nodemanager is running as process 2340746.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "harrisburg: nodemanager is running as process 1620452.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "des-moines: nodemanager is running as process 441463.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "denver: nodemanager is running as process 1399813.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "\n",
      "resourcemanager is running as process 703039.  Stop it first and ensure /tmp/hadoop-flarrieu-resourcemanager.pid file is empty before retry.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Yarn\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(resource_manager.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/start-yarn.sh') # Run start-yarn.sh on resource manager\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stopping Master and Workers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop DFS\n",
    "if password:\n",
    "    sure = input(\"Are you sure: y or n?\")\n",
    "    if sure == \"y\":\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh.connect(namenode.strip(), username=username, password=password)\n",
    "        stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/stop-dfs.sh') # Run stop-dfs.sh on namenode\n",
    "        print(stdout.read().decode())\n",
    "        print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Yarn\n",
    "if password:\n",
    "    sure = input(\"Are you sure: y or n?\")\n",
    "    if sure == \"y\":\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh.connect(resource_manager.strip(), username=username, password=password)\n",
    "        stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/stop-yarn.sh') # Run start-yarn.sh on resource manager\n",
    "        print(stdout.read().decode())\n",
    "        print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Starting Master and Workers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.deploy.master.Master running as process 23244.  Stop it first.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start master\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/spark/latest/sbin/start-master.sh') # Run start-master.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frankfort: org.apache.spark.deploy.worker.Worker running as process 419651.  Stop it first.\n",
      "frankfort: org.apache.spark.deploy.worker.Worker running as process 419776.  Stop it first.\n",
      "dover: org.apache.spark.deploy.worker.Worker running as process 2344384.  Stop it first.\n",
      "dover: org.apache.spark.deploy.worker.Worker running as process 2344532.  Stop it first.\n",
      "harrisburg: org.apache.spark.deploy.worker.Worker running as process 1626417.  Stop it first.\n",
      "harrisburg: org.apache.spark.deploy.worker.Worker running as process 1626582.  Stop it first.\n",
      "denver: org.apache.spark.deploy.worker.Worker running as process 1405900.  Stop it first.\n",
      "denver: org.apache.spark.deploy.worker.Worker running as process 1406144.  Stop it first.\n",
      "des-moines: org.apache.spark.deploy.worker.Worker running as process 442949.  Stop it first.\n",
      "des-moines: org.apache.spark.deploy.worker.Worker running as process 443072.  Stop it first.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start workers\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/spark/latest/sbin/start-workers.sh') # Run start-master.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .xml\n",
    "def parse_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    properties = {}\n",
    "    for property in root.findall('property'):\n",
    "        name = property.find('name').text\n",
    "        value = property.find('value').text\n",
    "        properties[name] = value\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .conf\n",
    "def parse_spark_conf(file_path):\n",
    "    properties = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if 'spark.master' in line and not \"#\" in line:\n",
    "                key, value = line.split(\" \", 1)\n",
    "                properties[key.strip()] = value.strip()\n",
    "                \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .env\n",
    "def parse_spark_env(file_path):\n",
    "    properties = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if 'export SPARK_MASTER_WEBUI_PORT' in line:\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                properties['port'] = value.split(\" \")[0]\n",
    "                pass\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS Client\n",
      "http://columbus-oh.cs.colostate.edu:30786\n",
      "Core Site\n",
      "hdfs://columbus-oh.cs.colostate.edu:30785\n",
      "Resource Management Client\n",
      "http://columbia.cs.colostate.edu:30798\n",
      "Spark\n",
      "spark://columbus-oh.cs.colostate.edu:30800\n",
      "Spark Client\n",
      "http://columbus-oh.cs.colostate.edu:30801\n"
     ]
    }
   ],
   "source": [
    "hdfs_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/hdfs-site.xml\"\n",
    "core_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/core-site.xml\"\n",
    "yarn_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/yarn-site.xml\"\n",
    "spark_conf_path = \"/s/chopin/a/grad/flarrieu/sparkConf/spark-defaults.conf\"\n",
    "spark_env_path = \"/s/chopin/a/grad/flarrieu/sparkConf/spark-env.sh\"\n",
    "\n",
    "hdfs_properties = parse_xml(hdfs_site_path)\n",
    "core_properties = parse_xml(core_site_path)\n",
    "yarn_properties = parse_xml(yarn_site_path)\n",
    "spark_properties = parse_spark_conf(spark_conf_path)\n",
    "spark_env_properties = parse_spark_env(spark_env_path)\n",
    "\n",
    "print(\"HDFS Client\")\n",
    "print(\"http://\" + hdfs_properties.get(\"dfs.namenode.http-address\"))\n",
    "\n",
    "print(\"Core Site\")\n",
    "print(core_properties.get('fs.default.name'))\n",
    "\n",
    "print(\"Resource Management Client\")\n",
    "print(\"http://\" + resource_manager.strip() + \".cs.colostate.edu:\" + yarn_properties.get('yarn.resourcemanager.webapp.address').split(\":\")[1])\n",
    "\n",
    "print(\"Spark\")\n",
    "print(spark_properties.get('spark.master'))\n",
    "\n",
    "print(\"Spark Client\")\n",
    "print(\"http://\" + namenode.strip() + \".cs.colostate.edu:\" + spark_env_properties.get(\"port\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
