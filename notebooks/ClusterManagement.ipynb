{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cluster Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooke is to assist with managing the your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Machines\n",
    "\n",
    "This is from the canvas announcement.\n",
    "\n",
    "### Freddy\n",
    "\n",
    "**Ports:** [30785, 30799] \n",
    "\n",
    "1. columbia \n",
    "2. columbus-oh \n",
    "3. concord \n",
    "4. denver \n",
    "5. des-moines \n",
    "6. dover \n",
    "7. frankfort \n",
    "8. harrisburg \n",
    "9. hartford \n",
    "10. helena \n",
    "11. honolulu \n",
    "12. indianapolis \n",
    "\n",
    "### Eric\n",
    "\n",
    "*idk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your user name\n",
    "username = 'flarrieu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columbus-oh\n",
      "concord\n",
      "columbia\n",
      "denver\n",
      "des-moines\n",
      "dover\n",
      "frankfort\n",
      "harrisburg\n",
      "30785\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Current Machines and port in use\n",
    "\n",
    "with open(f'/s/chopin/a/grad/{username}/bigdata_configs/machines_and_ports.txt') as f: \n",
    "    s = f.read()\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- namenode: columbus-oh\n",
      "\n",
      "- secondary namenode: concord\n",
      "\n",
      "- resource manager: columbia\n",
      "\n",
      "- datanode1: denver\n",
      "\n",
      "- datanode2: des-moines\n",
      "\n",
      "- datanode3: dover\n",
      "\n",
      "- datanode4: frankfort\n",
      "\n",
      "- datanode5: harrisburg\n"
     ]
    }
   ],
   "source": [
    "# Machine Roles After Cluster Setup\n",
    "\n",
    "with open(f'/s/chopin/a/grad/{username}/bigdata_configs/README.md', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "machine_roles = lines[-8:]\n",
    "\n",
    "for machine in machine_roles:\n",
    "    print(machine)\n",
    "    if \"namenode\" in machine and not \"secondary\" in machine:\n",
    "        namenode = machine.split(\" \")[2]\n",
    "    if \"resource manager\" in machine:\n",
    "        resource_manager = machine.split(\" \")[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Current Cluster Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/Modules/init/bash: line 104: unalias: pip3: not found\n",
      "/usr/share/Modules/init/bash: line 105: unalias: pip: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columbus-oh\n",
      "1836893 Jps\n",
      "142507 NameNode\n",
      "1258639 Master\n",
      "concord\n",
      "1078098 SecondaryNameNode\n",
      "1738807 Jps\n",
      "columbia\n",
      "761357 Jps\n",
      "2347557 ResourceManager\n",
      "denver\n",
      "3270048 NodeManager\n",
      "3897609 Worker\n",
      "3897437 Worker\n",
      "3269352 DataNode\n",
      "1624645 Jps\n",
      "des-moines\n",
      "405071 Jps\n",
      "175883 Worker\n",
      "175742 Worker\n",
      "dover\n",
      "2698953 DataNode\n",
      "3551319 Worker\n",
      "3793325 Jps\n",
      "3551171 Worker\n",
      "2699470 NodeManager\n",
      "frankfort\n",
      "573610 DataNode\n",
      "961685 Jps\n",
      "861857 Worker\n",
      "861729 Worker\n",
      "574052 NodeManager\n",
      "harrisburg\n",
      "2338883 NodeManager\n",
      "2338355 DataNode\n",
      "3637996 Jps\n",
      "3166939 Worker\n",
      "3166811 Worker\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Print the current status of the cluster\n",
    "# You will need to set the following in your ~/.bashrc: \n",
    "#   chmod +x /s/chopin/a/grad/{username}/hadoopConf/monitor.sh\n",
    "#   export MONITOR_CLUSTER=\"/s/chopin/a/grad/{username}/hadoopConf/monitor.sh\"\n",
    "\n",
    "source ~/.bashrc\n",
    "$MONITOR_CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Starting DFS and Yarn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: paramiko in /s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages (3.4.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages (from paramiko) (4.1.2)\n",
      "Requirement already satisfied: cryptography>=3.3 in /s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages (from paramiko) (42.0.5)\n",
      "Requirement already satisfied: pynacl>=1.5 in /s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages (from paramiko) (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/python-env/py39/lib/python3.9/site-packages (from cryptography>=3.3->paramiko) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/python-env/py39/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko) (2.21)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib64/python3.9/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/lib64/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib64/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/__main__.py\", line 31, in <module>\n",
      "    sys.exit(_main())\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 952, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2', new='24.0'),)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import paramiko\n",
    "\n",
    "# You will need to set the following in your ~/.bashrc: \n",
    "#   export PASSWORD=\"your password\"\n",
    "\n",
    "password = os.getenv(\"PASSWORD\")\n",
    "\n",
    "if not password:\n",
    "    print(\"Error: You need to set your password in you bashrc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [columbus-oh.cs.colostate.edu]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [concord.cs.colostate.edu]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start DFS\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/start-dfs.sh') # Run start-dfs.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting resourcemanager\n",
      "Starting nodemanagers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Yarn\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(resource_manager.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/start-yarn.sh') # Run start-yarn.sh on resource manager\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stopping Master and Workers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping org.apache.spark.deploy.master.Master\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if password:\n",
    "    sure = input(\"Are you sure: y or n?\")\n",
    "    if sure == \"y\":\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh.connect(namenode.strip(), username=username, password=password)\n",
    "        stdin, stdout, stderr = ssh.exec_command('bash /usr/local/spark/latest/sbin/stop-master.sh') # Run stop-dfs.sh on namenode\n",
    "        print(stdout.read().decode())\n",
    "        print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "des-moines: stopping org.apache.spark.deploy.worker.Worker\n",
      "des-moines: stopping org.apache.spark.deploy.worker.Worker\n",
      "frankfort: stopping org.apache.spark.deploy.worker.Worker\n",
      "frankfort: stopping org.apache.spark.deploy.worker.Worker\n",
      "denver: stopping org.apache.spark.deploy.worker.Worker\n",
      "denver: stopping org.apache.spark.deploy.worker.Worker\n",
      "harrisburg: stopping org.apache.spark.deploy.worker.Worker\n",
      "harrisburg: stopping org.apache.spark.deploy.worker.Worker\n",
      "dover: stopping org.apache.spark.deploy.worker.Worker\n",
      "dover: stopping org.apache.spark.deploy.worker.Worker\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if password:\n",
    "    sure = input(\"Are you sure: y or n?\")\n",
    "    if sure == \"y\":\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh.connect(resource_manager.strip(), username=username, password=password)\n",
    "        stdin, stdout, stderr = ssh.exec_command('bash /usr/local/spark/latest/sbin/stop-workers.sh') # Run start-yarn.sh on resource manager\n",
    "        print(stdout.read().decode())\n",
    "        print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Starting Master and Workers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.master.Master-1-columbus-oh.out\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start master\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/spark/latest/sbin/start-master.sh') # Run start-master.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denver: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-1-denver.out\n",
      "denver: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-2-denver.out\n",
      "frankfort: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-1-frankfort.out\n",
      "frankfort: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-2-frankfort.out\n",
      "harrisburg: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-1-harrisburg.out\n",
      "harrisburg: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-2-harrisburg.out\n",
      "dover: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-1-dover.out\n",
      "dover: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-2-dover.out\n",
      "des-moines: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-1-des-moines.out\n",
      "des-moines: starting org.apache.spark.deploy.worker.Worker, logging to /s/chopin/a/grad/flarrieu/sparkConf/logs/spark-flarrieu-org.apache.spark.deploy.worker.Worker-2-des-moines.out\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start workers\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/spark/latest/sbin/start-workers.sh') # Run start-master.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .xml\n",
    "def parse_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    properties = {}\n",
    "    for property in root.findall('property'):\n",
    "        name = property.find('name').text\n",
    "        value = property.find('value').text\n",
    "        properties[name] = value\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .conf\n",
    "def parse_spark_conf(file_path):\n",
    "    properties = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if 'spark.master' in line and not \"#\" in line:\n",
    "                key, value = line.split(\" \", 1)\n",
    "                properties[key.strip()] = value.strip()\n",
    "                \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .env\n",
    "def parse_spark_env(file_path):\n",
    "    properties = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if 'export SPARK_MASTER_WEBUI_PORT' in line:\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                properties['port'] = value.split(\" \")[0]\n",
    "                pass\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS Client\n",
      "http://columbus-oh.cs.colostate.edu:30786\n",
      "Core Site\n",
      "hdfs://columbus-oh.cs.colostate.edu:30785\n",
      "Resource Management Client\n",
      "http://columbia.cs.colostate.edu:30798\n",
      "Spark\n",
      "spark://columbus-oh.cs.colostate.edu:30800\n",
      "Spark Client\n",
      "http://columbus-oh.cs.colostate.edu:30801\n"
     ]
    }
   ],
   "source": [
    "hdfs_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/hdfs-site.xml\"\n",
    "core_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/core-site.xml\"\n",
    "yarn_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/yarn-site.xml\"\n",
    "spark_conf_path = \"/s/chopin/a/grad/flarrieu/sparkConf/spark-defaults.conf\"\n",
    "spark_env_path = \"/s/chopin/a/grad/flarrieu/sparkConf/spark-env.sh\"\n",
    "\n",
    "hdfs_properties = parse_xml(hdfs_site_path)\n",
    "core_properties = parse_xml(core_site_path)\n",
    "yarn_properties = parse_xml(yarn_site_path)\n",
    "spark_properties = parse_spark_conf(spark_conf_path)\n",
    "spark_env_properties = parse_spark_env(spark_env_path)\n",
    "\n",
    "print(\"HDFS Client\")\n",
    "print(\"http://\" + hdfs_properties.get(\"dfs.namenode.http-address\"))\n",
    "\n",
    "print(\"Core Site\")\n",
    "print(core_properties.get('fs.default.name'))\n",
    "\n",
    "print(\"Resource Management Client\")\n",
    "print(\"http://\" + resource_manager.strip() + \".cs.colostate.edu:\" + yarn_properties.get('yarn.resourcemanager.webapp.address').split(\":\")[1])\n",
    "\n",
    "print(\"Spark\")\n",
    "print(spark_properties.get('spark.master'))\n",
    "\n",
    "print(\"Spark Client\")\n",
    "print(\"http://\" + namenode.strip() + \".cs.colostate.edu:\" + spark_env_properties.get(\"port\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
