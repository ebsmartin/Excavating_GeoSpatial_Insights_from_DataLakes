{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cluster Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooke is to assist with managing the your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Machines\n",
    "\n",
    "This is from the canvas announcement.\n",
    "\n",
    "### Freddy\n",
    "\n",
    "**Ports:** [30785, 30799] \n",
    "\n",
    "1. columbia \n",
    "2. columbus-oh \n",
    "3. concord \n",
    "4. denver \n",
    "5. des-moines \n",
    "6. dover \n",
    "7. frankfort \n",
    "8. harrisburg \n",
    "9. hartford \n",
    "10. helena \n",
    "11. honolulu \n",
    "12. indianapolis \n",
    "\n",
    "### Eric\n",
    "\n",
    "**Ports:** [30181]\n",
    "\n",
    "1. hartford\n",
    "2. helena\n",
    "3. honolulu\n",
    "4. indianapolis\n",
    "5. des-moines\n",
    "6. dover\n",
    "7. frankfort\n",
    "8. harrisburg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your user name\n",
    "username = 'flarrieu'\n",
    "# username = 'ebmartin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found\n",
      "hartford\n",
      "helena\n",
      "honolulu\n",
      "indianapolis\n",
      "des-moines\n",
      "dover\n",
      "frankfort\n",
      "harrisburg\n",
      "30181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Current Machines and port in use\n",
    "\n",
    "try : \n",
    "    with open(f'/s/chopin/a/grad/{username}/bigdata_configs/machines_and_ports.txt') as f: \n",
    "        s = f.read()\n",
    "        print(s)\n",
    "except FileNotFoundError:\n",
    "    print('File not found')\n",
    "\n",
    "try :\n",
    "    with open(f'/s/chopin/l/grad/{username}/bigdata_configs/machines_and_ports.txt') as f: \n",
    "        s = f.read()\n",
    "        print(s)\n",
    "except FileNotFoundError:\n",
    "    print('File not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found\n",
      "- namenode: hartford\n",
      "\n",
      "- secondary namenode: helena\n",
      "\n",
      "- resource manager: honolulu\n",
      "\n",
      "- datanode1: indianapolis\n",
      "\n",
      "- datanode2: des-moines\n",
      "\n",
      "- datanode3: dover\n",
      "\n",
      "- datanode4: frankfort\n",
      "\n",
      "- datanode5: harrisburg\n"
     ]
    }
   ],
   "source": [
    "# Machine Roles After Cluster Setup\n",
    "\n",
    "try:\n",
    "    with open(f'/s/chopin/a/grad/{username}/bigdata_configs/README.md', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "except FileNotFoundError:\n",
    "    print('File not found')\n",
    "\n",
    "try:\n",
    "    with open(f'/s/chopin/l/grad/{username}/bigdata_configs/README.md', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "except FileNotFoundError:\n",
    "    print('File not found')\n",
    "    \n",
    "machine_roles = lines[-8:]\n",
    "\n",
    "for machine in machine_roles:\n",
    "    print(machine)\n",
    "    if \"namenode\" in machine and not \"secondary\" in machine:\n",
    "        namenode = machine.split(\" \")[2]\n",
    "    if \"resource manager\" in machine:\n",
    "        resource_manager = machine.split(\" \")[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Current Cluster Status**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/Modules/init/bash: line 113: unalias: pip3: not found\n",
      "/usr/share/Modules/init/bash: line 114: unalias: pip: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hartford\n",
      "273880 QuorumPeerMain\n",
      "1705892 Jps\n",
      "helena\n",
      "927550 UIServer\n",
      "927549 Nimbus\n",
      "799188 Jps\n",
      "honolulu\n",
      "3292581 Jps\n",
      "indianapolis\n",
      "1075349 Jps\n",
      "161556 Supervisor\n",
      "des-moines\n",
      "153094 Jps\n",
      "dover\n",
      "379412 Supervisor\n",
      "3488382 Jps\n",
      "frankfort\n",
      "166968 Supervisor\n",
      "841857 Jps\n",
      "harrisburg\n",
      "3146355 Jps\n",
      "191435 Supervisor\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Print the current status of the cluster\n",
    "# You will need to set the following in your ~/.bashrc: \n",
    "#   chmod +x /s/chopin/a/grad/{username}/hadoopConf/monitor.sh\n",
    "#   export MONITOR_CLUSTER=\"/s/chopin/a/grad/{username}/hadoopConf/monitor.sh\"\n",
    "\n",
    "source ~/.bashrc\n",
    "$MONITOR_CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Starting DFS and Yarn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: paramiko in /s/chopin/l/grad/ebmartin/.local/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: six in /s/chopin/l/grad/ebmartin/.local/lib/python3.9/site-packages (from paramiko) (1.16.0)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /s/chopin/l/grad/ebmartin/.local/lib/python3.9/site-packages (from paramiko) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /s/chopin/l/grad/ebmartin/.local/lib/python3.9/site-packages (from paramiko) (42.0.5)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /s/chopin/l/grad/ebmartin/.local/lib/python3.9/site-packages (from paramiko) (4.1.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/python-env/py39/lib/python3.9/site-packages (from cryptography>=2.5->paramiko) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/python-env/py39/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.5->paramiko) (2.21)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib64/python3.9/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip'\n",
      "Call stack:\n",
      "  File \"/usr/lib64/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib64/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/__main__.py\", line 31, in <module>\n",
      "    sys.exit(_main())\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 952, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/python-env/py39/lib/python3.9/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2', new='24.0'),)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: You need to set your password in you bashrc.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import paramiko\n",
    "\n",
    "# You will need to set the following in your ~/.bashrc: \n",
    "#   export PASSWORD=\"your password\"\n",
    "\n",
    "password = os.getenv(\"PASSWORD\")\n",
    "\n",
    "if not password:\n",
    "    print(\"Error: You need to set your password in you bashrc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(stdout\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(stderr\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m---> 10\u001b[0m \u001b[43mssh\u001b[49m\u001b[38;5;241m.\u001b[39mclose()    \n",
      "\u001b[0;31mNameError\u001b[0m: name 'ssh' is not defined"
     ]
    }
   ],
   "source": [
    "# Start DFS\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/start-dfs.sh') # Run start-dfs.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting resourcemanager\n",
      "Starting nodemanagers\n",
      "frankfort: nodemanager is running as process 418169.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "dover: nodemanager is running as process 2340746.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "harrisburg: nodemanager is running as process 1620452.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "des-moines: nodemanager is running as process 441463.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "denver: nodemanager is running as process 1399813.  Stop it first and ensure /tmp/hadoop-flarrieu-nodemanager.pid file is empty before retry.\n",
      "\n",
      "resourcemanager is running as process 703039.  Stop it first and ensure /tmp/hadoop-flarrieu-resourcemanager.pid file is empty before retry.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Yarn\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(resource_manager.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/start-yarn.sh') # Run start-yarn.sh on resource manager\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Stopping Master and Workers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop DFS\n",
    "if password:\n",
    "    sure = input(\"Are you sure: y or n?\")\n",
    "    if sure == \"y\":\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh.connect(namenode.strip(), username=username, password=password)\n",
    "        stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/stop-dfs.sh') # Run stop-dfs.sh on namenode\n",
    "        print(stdout.read().decode())\n",
    "        print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Yarn\n",
    "if password:\n",
    "    sure = input(\"Are you sure: y or n?\")\n",
    "    if sure == \"y\":\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh.connect(resource_manager.strip(), username=username, password=password)\n",
    "        stdin, stdout, stderr = ssh.exec_command('bash /usr/local/hadoop/latest/sbin/stop-yarn.sh') # Run start-yarn.sh on resource manager\n",
    "        print(stdout.read().decode())\n",
    "        print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Starting Master and Workers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.deploy.master.Master running as process 23244.  Stop it first.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start master\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/spark/latest/sbin/start-master.sh') # Run start-master.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frankfort: org.apache.spark.deploy.worker.Worker running as process 419651.  Stop it first.\n",
      "frankfort: org.apache.spark.deploy.worker.Worker running as process 419776.  Stop it first.\n",
      "dover: org.apache.spark.deploy.worker.Worker running as process 2344384.  Stop it first.\n",
      "dover: org.apache.spark.deploy.worker.Worker running as process 2344532.  Stop it first.\n",
      "harrisburg: org.apache.spark.deploy.worker.Worker running as process 1626417.  Stop it first.\n",
      "harrisburg: org.apache.spark.deploy.worker.Worker running as process 1626582.  Stop it first.\n",
      "denver: org.apache.spark.deploy.worker.Worker running as process 1405900.  Stop it first.\n",
      "denver: org.apache.spark.deploy.worker.Worker running as process 1406144.  Stop it first.\n",
      "des-moines: org.apache.spark.deploy.worker.Worker running as process 442949.  Stop it first.\n",
      "des-moines: org.apache.spark.deploy.worker.Worker running as process 443072.  Stop it first.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start workers\n",
    "if password:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(namenode.strip(), username=username, password=password)\n",
    "    stdin, stdout, stderr = ssh.exec_command('bash /usr/local/spark/latest/sbin/start-workers.sh') # Run start-master.sh on namenode\n",
    "    print(stdout.read().decode())\n",
    "    print(stderr.read().decode())\n",
    "\n",
    "ssh.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .xml\n",
    "def parse_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    properties = {}\n",
    "    for property in root.findall('property'):\n",
    "        name = property.find('name').text\n",
    "        value = property.find('value').text\n",
    "        properties[name] = value\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .conf\n",
    "def parse_spark_conf(file_path):\n",
    "    properties = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if 'spark.master' in line and not \"#\" in line:\n",
    "                key, value = line.split(\" \", 1)\n",
    "                properties[key.strip()] = value.strip()\n",
    "                \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing .env\n",
    "def parse_spark_env(file_path):\n",
    "    properties = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if 'export SPARK_MASTER_WEBUI_PORT' in line:\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                properties['port'] = value.split(\" \")[0]\n",
    "                pass\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS Client\n",
      "http://columbus-oh.cs.colostate.edu:30786\n",
      "Core Site\n",
      "hdfs://columbus-oh.cs.colostate.edu:30785\n",
      "Resource Management Client\n",
      "http://columbia.cs.colostate.edu:30798\n",
      "Spark\n",
      "spark://columbus-oh.cs.colostate.edu:30800\n",
      "Spark Client\n",
      "http://columbus-oh.cs.colostate.edu:30801\n"
     ]
    }
   ],
   "source": [
    "hdfs_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/hdfs-site.xml\"\n",
    "core_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/core-site.xml\"\n",
    "yarn_site_path = \"/s/chopin/a/grad/flarrieu/hadoopConf/yarn-site.xml\"\n",
    "spark_conf_path = \"/s/chopin/a/grad/flarrieu/sparkConf/spark-defaults.conf\"\n",
    "spark_env_path = \"/s/chopin/a/grad/flarrieu/sparkConf/spark-env.sh\"\n",
    "\n",
    "hdfs_properties = parse_xml(hdfs_site_path)\n",
    "core_properties = parse_xml(core_site_path)\n",
    "yarn_properties = parse_xml(yarn_site_path)\n",
    "spark_properties = parse_spark_conf(spark_conf_path)\n",
    "spark_env_properties = parse_spark_env(spark_env_path)\n",
    "\n",
    "print(\"HDFS Client\")\n",
    "print(\"http://\" + hdfs_properties.get(\"dfs.namenode.http-address\"))\n",
    "\n",
    "print(\"Core Site\")\n",
    "print(core_properties.get('fs.default.name'))\n",
    "\n",
    "print(\"Resource Management Client\")\n",
    "print(\"http://\" + resource_manager.strip() + \".cs.colostate.edu:\" + yarn_properties.get('yarn.resourcemanager.webapp.address').split(\":\")[1])\n",
    "\n",
    "print(\"Spark\")\n",
    "print(spark_properties.get('spark.master'))\n",
    "\n",
    "print(\"Spark Client\")\n",
    "print(\"http://\" + namenode.strip() + \".cs.colostate.edu:\" + spark_env_properties.get(\"port\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
