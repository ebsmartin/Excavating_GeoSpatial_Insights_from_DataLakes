{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Datset Spatial Partitioning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n",
      "/usr/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/local/python-env/py39/lib/python3.9/site-packages\")\n",
    "\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialze a SparkSession\n",
    "\n",
    "Initialize a test session to ensure the SparkSession is working properly. This will connect to the resource manager node that is running the YARN cluster. If we visit the YARN web portal, we can see that the Spark application is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring the pyspark library is being accessed from my local usr directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Sedona version: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "sedona_version = pkg_resources.get_distribution(\"apache-sedona\").version\n",
    "print(f\"Apache Sedona version: {sedona_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/latest\n",
      "/usr/bin/python3.9\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['PYSPARK_PYTHON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SedonaKepler import, verify if keplergl is installed\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark.sql.functions import year, to_json, expr, col, split, lit\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum as pyspark_sum\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import *\n",
    "import geopandas as gpd\n",
    "from py4j.java_gateway import java_import\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType, LongType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to make the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://artifacts.unidata.ucar.edu/repository/unidata-all added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /s/chopin/a/grad/flarrieu/.ivy2/cache\n",
      "The jars for the packages stored in: /s/chopin/a/grad/flarrieu/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-3.5_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cae7e06b-e6b3-4b7f-bd45-386b17a8f937;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-3.5_2.12;1.5.1 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/3.5.0-with-hadoop3.3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.sedona#sedona-common;1.5.1 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.19.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.google.guava#guava;25.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in user-list\n",
      "\tfound org.checkerframework#checker-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound com.uber#h3;4.1.1 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.10.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-spark-common-3.5_2.12;1.5.1 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in user-list\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.5.1-28.2 in central\n",
      ":: resolution report :: resolve 1069ms :: artifacts dl 46ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from user-list in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;25.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.uber#h3;4.1.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from user-list in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-common;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-3.5_2.12;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-common-3.5_2.12;1.5.1 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.5.1-28.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n",
      "\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   23  |   1   |   1   |   2   ||   21  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cae7e06b-e6b3-4b7f-bd45-386b17a8f937\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 21 already retrieved (0kB/28ms)\n",
      "24/04/22 10:44:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/tmp/ipykernel_1394388/990763263.py:22: DeprecationWarning: Call to deprecated function registerAll (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  SedonaRegistrator.registerAll(spark)\n",
      "/s/chopin/a/grad/flarrieu/.local/lib/python3.9/site-packages/sedona/register/geo_registrator.py:45: DeprecationWarning: Call to deprecated function register (Deprecated since 1.4.1, use SedonaContext.create() instead.).\n",
      "  cls.register(spark)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Dataset Spatial Partitioning') \\\n",
    "    .master('spark://columbus-oh.cs.colostate.edu:30800') \\\n",
    "    .config(\"spark.yarn.resourcemanager.address\", \"columbia.cs.colostate.edu:30799\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"500m\") \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .config('spark.jars.packages',\n",
    "            'org.apache.sedona:sedona-spark-3.5_2.12:1.5.1,'\n",
    "            'org.datasyslab:geotools-wrapper:1.5.1-28.2') \\\n",
    "    .config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to DEBUG\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# create a logger\n",
    "logger = spark._jvm.org.apache.log4j.LogManager.getLogger(__name__)\n",
    "logger.info(\"Pyspark initialized...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(df: DataFrame):\n",
    "    path = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "    # Here we use mode 'append' to add to the existing file\n",
    "    df.write.csv(path=path, mode='append', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_graph() -> DataFrame:\n",
    "    hdfs_path =  \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/graph/base_graph.csv\"\n",
    "    \n",
    "    # Define the schema for the graph data\n",
    "    schema = StructType([\n",
    "        StructField(\"Subject\", StringType(), True),\n",
    "        StructField(\"Relationship\", StringType(), True),\n",
    "        StructField(\"Object\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    graph_df = spark.read.csv(hdfs_path, header=True, schema=schema)\n",
    "    graph_df.show(truncate=False)\n",
    "    return graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_to_df(file_name: str): \n",
    "    data_directory = \"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/\"\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "\n",
    "    geojson_schema =  \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "    \n",
    "    df = spark.read.schema(geojson_schema).json(data_directory + file_name, multiLine=True)\n",
    "    \n",
    "    # Explode the features array to create a row for each feature and select the columns\n",
    "    df =  (df\n",
    "        .select(F.explode(\"features\").alias(\"features\"))\n",
    "        .select(\"features.*\")\n",
    "        # Use Sedona's ST_GeomFromGeoJSON function to convert the geometry string to a geometry object\n",
    "        .withColumn(\"geometry\", F.expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_files_recursive(path):\n",
    "    file_status_arr = fs.listStatus(spark._jvm.Path(path))\n",
    "    file_paths = []\n",
    "    \n",
    "    for file_status in file_status_arr:\n",
    "        if file_status.isDirectory():\n",
    "            file_paths += get_files_recursive(file_status.getPath().toString())\n",
    "        elif file_status.getPath().getName().endswith(('.geojson')):\n",
    "            file_paths.append(file_status.getPath().toString())\n",
    "    \n",
    "    print(file_paths)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory_into_df(directory_path: str):\n",
    "    file_dataframes = {}\n",
    "    files = get_files_recursive(directory_path)\n",
    "\n",
    "    for file_path in files:\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        \n",
    "        # Print the file path\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        geojson_schema = \"type string, crs string, totalFeatures long, features array<struct<type string, geometry string, properties map<string, string>>>\"\n",
    "\n",
    "        df = spark.read.schema(geojson_schema).json(file_path, multiLine=True)\n",
    "        \n",
    "        df = (df\n",
    "            .select(F.explode(\"features\").alias(\"features\"))\n",
    "            .select(\"features.*\")\n",
    "            .withColumn(\"geometry\", F.expr(\"ST_GeomFromGeoJSON(geometry)\"))\n",
    "            )\n",
    "        \n",
    "        file_dataframes[file_name] = df\n",
    "\n",
    "    return file_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Partition Dataset Based on States**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: States.geojson\n"
     ]
    }
   ],
   "source": [
    "df_states = get_file_to_df('States.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Wildfires.geojson\n"
     ]
    }
   ],
   "source": [
    "df_wildfires = get_file_to_df('Wildfires.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/MississippiWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/NorthCarolinaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/OklahomaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/VirginiaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/WestVirginiaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/LouisianaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 10:52:43 ERROR TaskSchedulerImpl: Lost executor 0 on 129.82.44.144: Command exited with code 50\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/MichiganWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/MassachusettsWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 11:01:42 ERROR TaskSchedulerImpl: Lost executor 4 on 129.82.44.141: Command exited with code 50\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/IdahoWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/FloridaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/NebraskaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/WashingtonWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/NewMexicoWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/PuertoRicoWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/SouthDakotaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/TexasWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/CaliforniaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 11:14:03 ERROR TaskSchedulerImpl: Lost executor 11 on 129.82.44.141: Command exited with code 50\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/AlabamaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/GeorgiaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/PennsylvaniaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 11:15:40 ERROR TaskSchedulerImpl: Lost executor 7 on 129.82.44.143: Command exited with code 50\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/MissouriWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/ColoradoWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/UtahWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/TennesseeWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/WyomingWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/NewYorkWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/KansasWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/AlaskaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/NevadaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/IllinoisWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/VermontWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/MontanaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/IowaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/SouthCarolinaWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON file has been successfully saved to HDFS at hdfs://columbus-oh.cs.colostate.edu:30785/WildfiresByState/NewHampshireWildfires.geojson.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "states = df_states.collect()\n",
    "for state in states:\n",
    "    state_name = state['properties']['NAME'].replace(\" \", '')\n",
    "    state_geometry = state['geometry']\n",
    "\n",
    "    filtered_wildfires = df_wildfires.filter(\n",
    "        expr(f\"ST_Contains(ST_GeomFromWKT('{state_geometry}'), ST_Centroid(geometry))\")\n",
    "    )\n",
    "\n",
    "    hdfs_output_path = f\"hdfs://columbus-oh.cs.colostate.edu:30785/geospatial/input/WildfiresByState/{state_name}Wildfires.geojson\"\n",
    "\n",
    "    # Attempt to save the DataFrame to HDFS as GeoJSON\n",
    "    try:\n",
    "        # Spark does not natively support writing to GeoJSON, so you might need to convert the DataFrame to JSON first\n",
    "        filtered_wildfires.write.format(\"json\").mode(\"overwrite\").save(hdfs_output_path)\n",
    "        print(f\"GeoJSON file has been successfully saved to HDFS at {hdfs_output_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write GeoJSON file for {state_name} to HDFS:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
